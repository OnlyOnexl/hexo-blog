<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="kubernetes调度器, love">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>kubernetes调度器 | love</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">love</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">love</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">kubernetes调度器</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/">
                                <span class="chip bg-color">调度器</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/k8s/" class="post-category">
                                k8s
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2022-05-17
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220114165454.png" alt="scheduling framework extensions"></p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><p>[TOC]</p>
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">实验环境：</span><br><span class="line">1、win10,vmwrokstation虚机；</span><br><span class="line">2、k8s集群：3台centos7.6 1810虚机，1个master节点,2个node节点</span><br><span class="line">   k8s version：v1.22.2</span><br><span class="line">   containerd://1.5.5</span><br></pre></td></tr></table></figure>

<h2 id="实验软件"><a href="#实验软件" class="headerlink" title="实验软件"></a>实验软件</h2><p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1P3Z_ujk22dYDXzM37WI5FA?pwd=v01w">https://pan.baidu.com/s/1P3Z_ujk22dYDXzM37WI5FA?pwd=v01w</a><br>提取码：v01w </p>
<p><code>2022.2.18-39.亲合性调度-实验代码.zip</code></p>
<p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1Nbkwz0rvDris8F6qFegJtw?pwd=4dx5">https://pan.baidu.com/s/1Nbkwz0rvDris8F6qFegJtw?pwd=4dx5</a><br>提取码：4dx5<br><code>2022.2.20-42.服务质量-实验代码</code></p>
<p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1CAC9j5yU-sg-aDfLPYVflg?pwd=b3jp">https://pan.baidu.com/s/1CAC9j5yU-sg-aDfLPYVflg?pwd=b3jp</a><br>提取码：b3jp<br><code>2022.2.19-40.拓扑分布约束-实验代码</code></p>
<p>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/19x43qNOI2oogXTcgEDCwgQ?pwd=0min">https://pan.baidu.com/s/19x43qNOI2oogXTcgEDCwgQ?pwd=0min</a><br>提取码：0min<br><code>2022.2.19-41.Descheduler-s实验代码</code></p>
<h2 id="本节实践"><a href="#本节实践" class="headerlink" title="本节实践"></a>本节实践</h2><ol>
<li>实践：nodeSelector测试(测试成功)-2022.5.16</li>
<li>实践：节点亲和性测试(测试成功)-2022.5.16</li>
<li>实践：pod亲和性(测试成功)-2022.5.16</li>
<li>实践：pod反亲和性(测试成功)-2022.5.16</li>
<li>实践：污点与容忍(测试成功)-2022.5.16</li>
<li>实践：nodeName测试(测试成功)-2022.5.16</li>
<li>实践：resources.requests测试(测试成功)-2022.5.17</li>
<li>实践：resources.limits测试(测试成功)-2022.5.17</li>
<li>实践：Qos解析(测试成功)-2022.5.17</li>
<li>实践：用拓扑分布约束来扩展daemonset功能(测试成功)-2022.5.17</li>
<li>实践：descheduler安装(测试成功)-2022.5.17</li>
</ol>
<h2 id="1、调度器"><a href="#1、调度器" class="headerlink" title="1、调度器"></a>1、调度器</h2><p><code>kube-scheduler</code> 是 kubernetes 的核心组件之一，主要负责整个集群资源的调度功能，根据<strong>特定的调度算法和策略</strong>，将 Pod 调度到最优的工作节点上面去，从而更加合理、更加充分的利用集群的资源，这也是我们选择使用 kubernetes 一个非常重要的理由。如果一门新的技术不能帮助企业节约成本、提供效率，我相信是很难推进的。</p>
<p>🍂 什么是调度？</p>
<p>调度，可以说是Kubernetes最核心的功能之一了。</p>
<p>Pod是Kubernetes中最小的调度单元，而Pod又是运行在Node之上的。所谓调度，简单来说就是为一个新创建出来的 Pod，寻找一个最适合它运行的Node。 </p>
<p>🍂 调度概览</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514111718.png"></p>
<p>🍂 调度器分类</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514220339.png"><br>1.单体调度器：对于大规模批量调度诉求场景，不能胜任！(基于pod的事件调度)！<br>2.两层调度器：应用平台–hadoop,spark；资源调度器(负责底层计算资源的管理)，应用调度器；resource offers，存在的问题：1.资源争抢如何解决？2.分配资源不合理如何处理  解决办法：悲观锁 先锁定资源，再进行资源的腾挪处理。–&gt;效率不高<br>3.状态共享调度器：基于版本控制/事务控制的基于乐观锁的调度！ full state,本地缓存，回写，冲突判断，重试。</p>
<h2 id="2、调度流程"><a href="#2、调度流程" class="headerlink" title="2、调度流程"></a>2、调度流程</h2><h3 id="1-默认调度器"><a href="#1-默认调度器" class="headerlink" title="1.默认调度器"></a>1.默认调度器</h3><p><code>kube-scheduler</code></p>
<blockquote>
<p>kube-scheduler负责分配调度Pod 到集群内的节点上，它监听kube-apiserver，查询还未分配Node的Pod，然后根据调度策略为这些Pod分配节点（更新Pod 的NodeName字段)。(生产者-消费者模型，plugin)</p>
</blockquote>
<p>默认情况下，<code>kube-scheduler</code> 提供的默认调度器能够满足我们绝大多数的要求，我们前面和大家接触的示例也基本上用的<strong>默认的策略</strong>，都可以保证我们的 Pod 可以被分配到资源充足的节点上运行。但是在实际的线上项目中，可能我们自己会比 kubernetes 更加了解我们自己的应用，比如我们希望一个 Pod 只能运行在特定的几个节点上，或者这几个节点只能用来运行特定类型的应用，这就<strong>需要我们的调度器能够可控</strong>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">例如：随便导出一个pod的yaml文件，可以看到其默认调度器就是`default-scheduler`.</span><br><span class="line">$ kubectl get po apisix-etcd-0 -napisix -oyaml</span><br><span class="line">……</span><br><span class="line">schedulerName: default-scheduler</span><br><span class="line">……</span><br><span class="line"></span><br><span class="line"><span class="comment">#也就是k8s集群默认使用的：</span></span><br><span class="line">$ kubectl get po -A</span><br><span class="line">……</span><br><span class="line">kube-system            kube-scheduler-master1                             1/1     Running    3 (27d ago)   108d</span><br></pre></td></tr></table></figure>



<p><code>kube-scheduler</code> 的主要作用就是根据特定的<strong>调度算法和调度策略</strong>将 Pod 调度到合适的 Node 节点上去，<strong>是一个独立的二进制程序</strong>，<strong>启动之后会一直监听 API Server，获取到 <code>PodSpec.NodeName</code> 为空的 Pod，对每个 Pod 都会创建一个 binding。</strong></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220114165101.png" alt="kube-scheduler structrue"></p>
<p>这个过程在我们看来好像比较简单，但在实际的生产环境中，需要考虑的问题就有很多了：</p>
<ul>
<li>如何保证全部的节点调度的公平性？要知道并不是所有节点资源配置一定都是一样的</li>
<li>如何保证每个节点都能被分配资源？</li>
<li>集群资源如何能够被高效利用？</li>
<li>集群资源如何才能被最大化使用？</li>
<li><strong>如何保证 Pod 调度的性能和效率？</strong>(假设说有1w个节点，我是否可以在其中1k个节点上进行筛选呢，这样就可以大幅度提高调度效率了😀)</li>
<li>用户是否可以根据自己的实际需求定制自己的调度策略？</li>
</ul>
<p>🍂 调度主要分为以下几个部分：</p>
<ul>
<li>首先是<strong>预选过程</strong>，过滤掉不满足条件的节点，这个过程称为 <code>Predicates</code>（过滤）</li>
<li>然后是<strong>优选过程</strong>，对通过的节点按照优先级排序，称之为 <code>Priorities</code>（打分）</li>
<li>最后从中选择优先级最高的节点，如果中间任何一步骤有错误，就直接返回错误</li>
</ul>
<p><code>Predicates</code> 阶段首先遍历全部节点，过滤掉不满足条件的节点，属于<code>强制性</code>规则，这一阶段输出的所有满足要求的节点将被记录并作为第二阶段的输入，<strong>如果所有的节点都不满足条件，那么 Pod 将会一直处于 Pending 状态，直到有节点满足条件，在这期间调度器会不断的重试。</strong></p>
<p><u>所以我们在部署应用的时候，如果发现有 Pod 一直处于 Pending 状态，那么就是没有满足调度条件的节点，这个时候可以去检查下节点资源是否可用。</u></p>
<p><code>Priorities</code> 阶段即再次对节点进行筛选，如果有多个节点都满足条件的话，那么系统会按照节点的优先级(<code>priorites</code>)大小对节点进行排序，最后选择优先级最高的节点来部署 Pod 应用。</p>
<blockquote>
<p>01、如果你的pod处于pending状态，那么一定就是调度器出现了问题，那么原因会很多，有可能是你的node资源不足，有可能是你的节点已经被占用了……(因此需要使用kubectl describle pod xxx来查看原因)</p>
<p>02、所谓的bing操作就是如下：<br>$ kubectl get po apisix-etcd-0 -napisix -oyaml<br>……<br>nodeName: node2 #将pod的配置清单的nodeName字段补充完成。<br>……</p>
</blockquote>
<p>🍂 下面是调度过程的简单示意图：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220114165333.png" alt="kube-scheduler filter"></p>
<p>更详细的流程是这样的：</p>
<ul>
<li><p>首先，客户端通过 API Server 的 REST API 或者 kubectl 工具创建 Pod 资源</p>
</li>
<li><p>API Server 收到用户请求后，存储相关数据到 etcd 数据库中</p>
</li>
<li><p>调度器监听 API Server 查看到还未被调度(bind)的 Pod 列表，循环遍历地为每个 Pod 尝试分配节点，这个分配过程就是我们上面提到的两个阶段：</p>
<ul>
<li><strong>预选阶段(Predicates)<strong>，过滤节点，调度器用</strong>一组规则</strong>过滤掉不符合要求的 Node 节点，比如 Pod 设置了资源的 request，那么可用资源比 Pod 需要的资源少的主机显然就会被过滤掉。</li>
<li><strong>优选阶段(Priorities)<strong>，为节点的优先级打分，将上一阶段过滤出来的 Node 列表进行打分，</strong>调度器会考虑一些整体的优化策略</strong>，比如把 Deployment 控制的多个 Pod 副本尽量分布到不同的主机上，使用最低负载的主机等等策略。</li>
</ul>
</li>
<li><p>经过上面的阶段过滤后选择打分最高的 Node 节点和 Pod 进行 <code>binding</code> 操作，然后将结果存储到 etcd 中， 最后被选择出来的 Node 节点对应的 kubelet 去执行创建 Pod 的相关操作（当然也是 watch APIServer 发现的）。</p>
</li>
</ul>
<p>🍂 Predicates plugin工作原理</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514230119.png"><br>链式过滤器</p>
<p>🍂 调度插件</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514225924.png"><br>LeastAllocated：空闲资源多的分高  –使的node上的负载比较合理一点！<br>MostAllocated：空闲资源少的分高  – 可以退回Node资源！</p>
<h3 id="2-扩展调度器"><a href="#2-扩展调度器" class="headerlink" title="2.扩展调度器"></a>2.扩展调度器</h3><p>包括很多时候，默认的调度器已经不能满足业务需求，需要对它做<strong>自定义的扩展和实现</strong>，具体该怎么做，背后的原理又是什么样的，业界有没有优秀案例可供参考，这些都是非常令人头疼的问题。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514231649.png"></p>
<p>extender本身就是一个拉低性能的因素。</p>
<p>🍂 考虑到实际环境中的各种复杂情况，<strong>kubernetes 的调度器采用插件化的形式实现</strong>，可以方便用户进行定制或者二次开发，我们可以自定义一个调度器并以插件形式和 kubernetes 进行集成。</p>
<blockquote>
<p>开发人员注意即可：</p>
<p>kubernetes 调度器的源码位于 <code>kubernetes/pkg/scheduler</code> 中，其中 Scheduler 创建和运行的核心程序，对应的代码在 <code>pkg/scheduler/scheduler.go</code>，如果要查看 <code>kube-scheduler</code> 的入口程序，对应的代码在 <code>cmd/kube-scheduler/scheduler.go</code>。</p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://github1s.com/kubernetes/kubernetes/tree/v1.22.5">https://github1s.com/kubernetes/kubernetes/tree/v1.22.5</a></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220216165230058.png" alt="image-20220216165230058"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220216165341841.png" alt="image-20220216165341841"></p>
<h3 id="3-调度框架"><a href="#3-调度框架" class="headerlink" title="3.调度框架"></a>3.调度框架</h3><p><strong>基于Scheduler Framework实现扩展</strong></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514231731.png"></p>
<p>未来主流的方法。</p>
<p>🍂 <strong>目前调度器已经全部通过插件的方式实现了调度框架</strong>，默认开启的调度插件如以下代码所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/algorithmprovider/registry.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">getDefaultConfig</span><span class="params">()</span> *<span class="title">schedulerapi</span>.<span class="title">Plugins</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> &amp;schedulerapi.Plugins&#123;</span><br><span class="line">        QueueSort: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: queuesort.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        PreFilter: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: noderesources.FitName&#125;,</span><br><span class="line">                &#123;Name: nodeports.Name&#125;,</span><br><span class="line">                &#123;Name: podtopologyspread.Name&#125;,</span><br><span class="line">                &#123;Name: interpodaffinity.Name&#125;,</span><br><span class="line">                &#123;Name: volumebinding.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        Filter: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: nodeunschedulable.Name&#125;,</span><br><span class="line">                &#123;Name: noderesources.FitName&#125;,</span><br><span class="line">                &#123;Name: nodename.Name&#125;,</span><br><span class="line">                &#123;Name: nodeports.Name&#125;,</span><br><span class="line">                &#123;Name: nodeaffinity.Name&#125;,</span><br><span class="line">                &#123;Name: volumerestrictions.Name&#125;,</span><br><span class="line">                &#123;Name: tainttoleration.Name&#125;,</span><br><span class="line">                &#123;Name: nodevolumelimits.EBSName&#125;,</span><br><span class="line">                &#123;Name: nodevolumelimits.GCEPDName&#125;,</span><br><span class="line">                &#123;Name: nodevolumelimits.CSIName&#125;,</span><br><span class="line">                &#123;Name: nodevolumelimits.AzureDiskName&#125;,</span><br><span class="line">                &#123;Name: volumebinding.Name&#125;,</span><br><span class="line">                &#123;Name: volumezone.Name&#125;,</span><br><span class="line">                &#123;Name: podtopologyspread.Name&#125;,</span><br><span class="line">                &#123;Name: interpodaffinity.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        PostFilter: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: defaultpreemption.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        PreScore: &amp;schedulerapi.PluginSet&#123; #打分</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: interpodaffinity.Name&#125;,</span><br><span class="line">                &#123;Name: podtopologyspread.Name&#125;,</span><br><span class="line">                &#123;Name: tainttoleration.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        Score: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: noderesources.BalancedAllocationName, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;Name: imagelocality.Name, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;Name: interpodaffinity.Name, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;Name: noderesources.LeastAllocatedName, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;Name: nodeaffinity.Name, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">                &#123;Name: nodepreferavoidpods.Name, Weight: <span class="number">10000</span>&#125;,</span><br><span class="line">                <span class="comment">// Weight is doubled because:</span></span><br><span class="line">                <span class="comment">// - This is a score coming from user preference.</span></span><br><span class="line">                <span class="comment">// - It makes its signal comparable to NodeResourcesLeastAllocated.</span></span><br><span class="line">                &#123;Name: podtopologyspread.Name, Weight: <span class="number">2</span>&#125;,</span><br><span class="line">                &#123;Name: tainttoleration.Name, Weight: <span class="number">1</span>&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        Reserve: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: volumebinding.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        PreBind: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: volumebinding.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">        Bind: &amp;schedulerapi.PluginSet&#123;</span><br><span class="line">            Enabled: []schedulerapi.Plugin&#123;</span><br><span class="line">                &#123;Name: defaultbinder.Name&#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面我们可以看出调度器的一系列算法由各种插件在调度的不同阶段来完成，下面我们就先来了解下调度框架。</p>
<p>调度框架定义了一组扩展点，用户可以实现扩展点定义的接口来定义自己的调度逻辑（我们称之为<strong>扩展</strong>），并将扩展注册到扩展点上，调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。调度框架在预留扩展点时，都是有特定的目的，有些扩展点上的扩展可以改变调度程序的决策方法，有些扩展点上的扩展只是发送一个通知。</p>
<p>我们知道每当调度一个 Pod 时，都会按照两个过程来执行：<strong>调度过程和绑定过程</strong>。</p>
<p>调度过程为 Pod 选择一个合适的节点，绑定过程则将调度过程的决策应用到集群中（也就是在被选定的节点上运行 Pod），将调度过程和绑定过程合在一起，称之为<strong>调度上下文（scheduling context）</strong>。需要注意的是**<u>调度过程是<code>同步</code>运行的（同一时间点只为一个 Pod 进行调度）</u>**，绑定过程可异步运行（同一时间点可并发为多个 Pod 执行绑定）。</p>
<p>调度过程和绑定过程遇到如下情况时会中途退出：</p>
<ul>
<li>调度程序认为当前没有该 Pod 的可选节点</li>
<li>内部错误</li>
</ul>
<p>这个时候，该 Pod 将被放回到 <strong>待调度队列</strong>，并等待下次重试。</p>
<h4 id="1-扩展点（Extension-Points）"><a href="#1-扩展点（Extension-Points）" class="headerlink" title="1.扩展点（Extension Points）"></a>1.扩展点（Extension Points）</h4><p>下图展示了调度框架中的调度上下文及其中的扩展点，一个扩展可以注册多个扩展点，以便可以执行更复杂的有状态的任务。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220114165454.png" alt="scheduling framework extensions"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514222039.png"></p>
<p>调度阶段：<br>    1.predicate(预选)：<br>    2.priority/score(优选)：<br>绑定阶段：</p>
<p>🍂 详细过程：</p>
<ol>
<li><code>QueueSort</code> 扩展用于对 Pod 的待调度队列进行排序，以决定先调度哪个 Pod，<code>QueueSort</code> 扩展本质上只需要实现一个方法 <code>Less(Pod1, Pod2)</code> 用于比较两个 Pod 谁更优先获得调度即可，同一时间点只能有一个 <code>QueueSort</code> 插件生效。</li>
<li><code>Pre-filter</code> 扩展用于<strong>对 Pod 的信息进行预处理</strong>，或者检查一些集群或 Pod 必须满足的前提条件，如果 <code>pre-filter</code> 返回了 error，则调度过程终止。</li>
<li><code>Filter</code> 扩展用于排除那些不能运行该 Pod 的节点，对于每一个节点，调度器将按顺序执行 <code>filter</code> 扩展；如果任何一个 <code>filter</code> 将节点标记为不可选，则余下的 <code>filter</code> 扩展将不会被执行。<strong>调度器可以同时对多个节点执行 <code>filter</code> 扩展。</strong></li>
<li><code>Post-filter</code> <strong>是一个通知类型的扩展点</strong>，调用该扩展的参数是 <code>filter</code> 阶段结束后被筛选为<strong>可选节点</strong>的节点列表，可以在扩展中使用这些信息更新内部状态，或者产生日志或 metrics 信息。</li>
<li><code>Scoring</code> 扩展用于为所有可选节点进行打分，调度器将针对每一个节点调用 <code>Soring</code> 扩展，评分结果是一个范围内的整数。在 <code>normalize scoring</code> 阶段，调度器将会把每个 <code>scoring</code> 扩展<strong>对具体某个节点的评分结果和该扩展的权重合并起来</strong>，作为最终评分结果。</li>
<li><code>Normalize scoring</code> <strong>扩展在调度器对节点进行最终排序之前修改每个节点的评分结果</strong>，注册到该扩展点的扩展在被调用时，将获得同一个插件中的 <code>scoring</code> 扩展的评分结果作为参数，调度框架每执行一次调度，都将调用所有插件中的一个 <code>normalize scoring</code> 扩展一次。</li>
<li><code>Reserve</code> 是一个通知性质的扩展点，有状态的插件可以使用该扩展点来获得节点上为 Pod 预留的资源，该事件发生在调度器将 Pod 绑定到节点之前，目的是避免调度器在等待 Pod 与节点绑定的过程中调度新的 Pod 到节点上时，发生实际使用资源超出可用资源的情况（<strong>因为绑定 Pod 到节点上是异步发生的</strong>）。这是调度过程的最后一个步骤，Pod 进入 reserved 状态以后，要么在绑定失败时触发 Unreserve 扩展，要么在绑定成功时，由 Post-bind 扩展结束绑定过程。</li>
<li><code>Permit</code> 扩展用于阻止或者延迟 Pod 与节点的绑定。Permit 扩展可以做下面三件事中的一项：<ul>
<li>approve（批准）：当所有的 permit 扩展都 approve 了 Pod 与节点的绑定，调度器将继续执行绑定过程</li>
<li>deny（拒绝）：如果任何一个 permit 扩展 deny 了 Pod 与节点的绑定，Pod 将被放回到待调度队列，此时将触发 <code>Unreserve</code> 扩展。</li>
<li>wait（等待）：如果一个 permit 扩展返回了 wait，则 Pod 将保持在 permit 阶段，直到被其他扩展 approve，如果超时事件发生，wait 状态变成 deny，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展</li>
</ul>
</li>
<li><code>Pre-bind</code> 扩展用于在 Pod 绑定之前执行某些逻辑。例如，pre-bind 扩展可以将一个基于网络的数据卷挂载到节点上，以便 Pod 可以使用。如果任何一个 <code>pre-bind</code> 扩展返回错误，Pod 将被放回到待调度队列，此时将触发 Unreserve 扩展。</li>
<li><code>Bind</code> 扩展用于将 Pod 绑定到节点上：<ul>
<li>只有所有的 pre-bind 扩展都成功执行了，bind 扩展才会执行</li>
<li>调度框架按照 bind 扩展注册的顺序逐个调用 bind 扩展</li>
<li>具体某个 bind 扩展可以选择处理或者不处理该 Pod</li>
<li>如果某个 bind 扩展处理了该 Pod 与节点的绑定，余下的 bind 扩展将被忽略</li>
</ul>
</li>
<li><code>Post-bind</code> 是一个通知性质的扩展：<ul>
<li>Post-bind 扩展在 Pod 成功绑定到节点上之后被动调用</li>
<li>Post-bind 扩展是绑定过程的最后一个步骤，可以用来执行<strong>资源清理</strong>的动作</li>
</ul>
</li>
<li><code>Unreserve</code> 是一个通知性质的扩展，如果为 Pod 预留了资源，Pod 又在被绑定过程中被拒绝绑定，则 unreserve 扩展将被调用。Unreserve 扩展应该释放已经为 Pod 预留的节点上的计算资源。<strong>在一个插件中，reserve 扩展和 unreserve 扩展应该成对出现</strong>。</li>
</ol>
<p>🍂 如果我们要实现自己的插件，必须向调度框架注册插件并完成配置，另外还必须实现<strong>扩展点接口</strong>，对应的扩展点接口我们可以在源码 <code>pkg/scheduler/framework/v1alpha1/interface.go</code> 文件中找到，如下所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Plugin is the parent type for all the scheduling framework plugins.</span></span><br><span class="line"><span class="keyword">type</span> Plugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Name() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> QueueSortPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Less(*PodInfo, *PodInfo) <span class="keyword">bool</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreFilterPlugin is an interface that must be implemented by &quot;prefilter&quot; plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called at the beginning of the scheduling cycle.</span></span><br><span class="line"><span class="keyword">type</span> PreFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PreFilter(pc *PluginContext, p *v1.Pod) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// FilterPlugin is an interface for Filter plugins. These plugins are called at the</span></span><br><span class="line"><span class="comment">// filter extension point for filtering out hosts that cannot run a pod.</span></span><br><span class="line"><span class="comment">// This concept used to be called &#x27;predicate&#x27; in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return &quot;Success&quot;, &quot;Unschedulable&quot; or &quot;Error&quot; in Status.code.</span></span><br><span class="line"><span class="comment">// However, the scheduler accepts other valid codes as well.</span></span><br><span class="line"><span class="comment">// Anything other than &quot;Success&quot; will lead to exclusion of the given host from</span></span><br><span class="line"><span class="comment">// running the pod.</span></span><br><span class="line"><span class="keyword">type</span> FilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Filter(pc *PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostFilterPlugin is an interface for Post-filter plugin. Post-filter is an</span></span><br><span class="line"><span class="comment">// informational extension point. Plugins will be called with a list of nodes</span></span><br><span class="line"><span class="comment">// that passed the filtering phase. A plugin may use this data to update internal</span></span><br><span class="line"><span class="comment">// state or to generate logs/metrics.</span></span><br><span class="line"><span class="keyword">type</span> PostFilterPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PostFilter(pc *PluginContext, pod *v1.Pod, nodes []*v1.Node, filteredNodesStatuses NodeToStatusMap) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScorePlugin is an interface that must be implemented by &quot;score&quot; plugins to rank</span></span><br><span class="line"><span class="comment">// nodes that passed the filtering phase.</span></span><br><span class="line"><span class="keyword">type</span> ScorePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Score(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (<span class="keyword">int</span>, *Status)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScoreWithNormalizePlugin is an interface that must be implemented by &quot;score&quot;</span></span><br><span class="line"><span class="comment">// plugins that also need to normalize the node scoring results produced by the same</span></span><br><span class="line"><span class="comment">// plugin&#x27;s &quot;Score&quot; method.</span></span><br><span class="line"><span class="keyword">type</span> ScoreWithNormalizePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    ScorePlugin</span><br><span class="line">    NormalizeScore(pc *PluginContext, p *v1.Pod, scores NodeScoreList) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ReservePlugin is an interface for Reserve plugins. These plugins are called</span></span><br><span class="line"><span class="comment">// at the reservation point. These are meant to update the state of the plugin.</span></span><br><span class="line"><span class="comment">// This concept used to be called &#x27;assume&#x27; in the original scheduler.</span></span><br><span class="line"><span class="comment">// These plugins should return only Success or Error in Status.code. However,</span></span><br><span class="line"><span class="comment">// the scheduler accepts other valid codes as well. Anything other than Success</span></span><br><span class="line"><span class="comment">// will lead to rejection of the pod.</span></span><br><span class="line"><span class="keyword">type</span> ReservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Reserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PreBindPlugin is an interface that must be implemented by &quot;prebind&quot; plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod being scheduled.</span></span><br><span class="line"><span class="keyword">type</span> PreBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PreBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PostBindPlugin is an interface that must be implemented by &quot;postbind&quot; plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called after a pod is successfully bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PostBindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    PostBind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// UnreservePlugin is an interface for Unreserve plugins. This is an informational</span></span><br><span class="line"><span class="comment">// extension point. If a pod was reserved and then rejected in a later phase, then</span></span><br><span class="line"><span class="comment">// un-reserve plugins will be notified. Un-reserve plugins should clean up state</span></span><br><span class="line"><span class="comment">// associated with the reserved Pod.</span></span><br><span class="line"><span class="keyword">type</span> UnreservePlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Unreserve(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// PermitPlugin is an interface that must be implemented by &quot;permit&quot; plugins.</span></span><br><span class="line"><span class="comment">// These plugins are called before a pod is bound to a node.</span></span><br><span class="line"><span class="keyword">type</span> PermitPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Permit(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) (*Status, time.Duration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// BindPlugin is an interface that must be implemented by &quot;bind&quot; plugins. Bind</span></span><br><span class="line"><span class="comment">// plugins are used to bind a pod to a Node.</span></span><br><span class="line"><span class="keyword">type</span> BindPlugin <span class="keyword">interface</span> &#123;</span><br><span class="line">    Plugin</span><br><span class="line">    Bind(pc *PluginContext, p *v1.Pod, nodeName <span class="keyword">string</span>) *Status</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>🍂 对于调度框架插件的启用或者禁用，我们可以使用安装集群时的 <a target="_blank" rel="noopener" href="https://godoc.org/k8s.io/kubernetes/pkg/scheduler/apis/config#KubeSchedulerConfiguration">KubeSchedulerConfiguration</a> 资源对象来进行配置。下面的例子中的配置启用了一个实现了 <code>reserve</code> 和 <code>preBind</code> 扩展点的插件，并且禁用了另外一个插件，同时为插件 foo 提供了一些配置信息：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line">  <span class="attr">reserve:</span></span><br><span class="line">    <span class="attr">enabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="attr">disabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">baz</span></span><br><span class="line">  <span class="attr">preBind:</span></span><br><span class="line">    <span class="attr">enabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">disabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">baz</span></span><br><span class="line"></span><br><span class="line"><span class="attr">pluginConfig:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">  <span class="attr">args:</span> <span class="string">&gt;</span></span><br><span class="line">    <span class="string">foo插件可以解析的任意内容</span></span><br></pre></td></tr></table></figure>



<p>🍂 扩展的调用顺序如下：</p>
<ul>
<li>如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展</li>
<li>如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展</li>
<li>默认插件的扩展始终被最先调用，然后按照 <code>KubeSchedulerConfiguration</code> 中扩展的激活 <code>enabled</code> 顺序逐个调用扩展点的扩展</li>
<li>可以先禁用默认插件的扩展，然后在 <code>enabled</code> 列表中的某个位置激活默认插件的扩展，这种做法可以改变默认插件的扩展被调用时的顺序</li>
</ul>
<p>假设默认插件 foo 实现了 <code>reserve</code> 扩展点，此时我们要添加一个插件 bar，想要在 foo 之前被调用，则应该先禁用 foo 再按照 bar foo 的顺序激活。示例配置如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="string">...</span></span><br><span class="line"></span><br><span class="line"><span class="attr">plugins:</span></span><br><span class="line">  <span class="attr">reserve:</span></span><br><span class="line">    <span class="attr">enabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">disabled:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">foo</span></span><br></pre></td></tr></table></figure>

<p>在源码目录 <code>pkg/scheduler/framework/plugins/examples</code> 中有几个示范插件，我们可以参照其实现方式。</p>
<h4 id="2-示例-代码部分"><a href="#2-示例-代码部分" class="headerlink" title="2.示例(代码部分)"></a>2.示例(代码部分)</h4><blockquote>
<p>因为涉及到代码部分，本次这里不做演示，看下就好。</p>
</blockquote>
<p>其实要实现一个调度框架的插件，并不难，我们只要实现对应的扩展点，然后将插件注册到调度器中即可，下面是默认调度器在初始化的时候注册的插件：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pkg/scheduler/algorithmprovider/registry.go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewRegistry</span><span class="params">()</span> <span class="title">Registry</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> Registry&#123;</span><br><span class="line">        <span class="comment">// FactoryMap:</span></span><br><span class="line">        <span class="comment">// New plugins are registered here.</span></span><br><span class="line">        <span class="comment">// example:</span></span><br><span class="line">        <span class="comment">// &#123;</span></span><br><span class="line">        <span class="comment">//  stateful_plugin.Name: stateful.NewStatefulMultipointExample,</span></span><br><span class="line">        <span class="comment">//  fooplugin.Name: fooplugin.New,</span></span><br><span class="line">        <span class="comment">// &#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但是可以看到默认并没有注册一些插件，所以要想让调度器能够识别我们的插件代码，就需要自己来实现一个调度器了，当然这个调度器我们完全没必要完全自己实现，直接调用默认的调度器，然后在上面的 <code>NewRegistry()</code> 函数中将我们的插件注册进去即可。在 <code>kube-scheduler</code> 的源码文件 <code>kubernetes/cmd/kube-scheduler/app/server.go</code> 中有一个 <code>NewSchedulerCommand</code> 入口函数，其中的参数是一个类型为 <code>Option</code> 的列表，而这个 <code>Option</code> 恰好就是一个插件配置的定义：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Option configures a framework.Registry.</span></span><br><span class="line"><span class="keyword">type</span> Option <span class="function"><span class="keyword">func</span><span class="params">(framework.Registry)</span> <span class="title">error</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// NewSchedulerCommand creates a *cobra.Command object with default parameters and registryOptions</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewSchedulerCommand</span><span class="params">(registryOptions ...Option)</span> *<span class="title">cobra</span>.<span class="title">Command</span></span> &#123;</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所以我们完全就可以直接调用这个函数来作为我们的函数入口，并且传入我们自己实现的插件作为参数即可，而且该文件下面还有一个名为 <code>WithPlugin</code> 的函数可以来创建一个 <code>Option</code> 实例：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// WithPlugin creates an Option based on plugin name and factory.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">WithPlugin</span><span class="params">(name <span class="keyword">string</span>, factory framework.PluginFactory)</span> <span class="title">Option</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="function"><span class="keyword">func</span><span class="params">(registry framework.Registry)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">        <span class="keyword">return</span> registry.Register(name, factory)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>所以最终我们的入口函数如下所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    rand.Seed(time.Now().UTC().UnixNano())</span><br><span class="line"></span><br><span class="line">    command := app.NewSchedulerCommand(</span><br><span class="line">        app.WithPlugin(sample.Name, sample.New),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    logs.InitLogs()</span><br><span class="line">    <span class="keyword">defer</span> logs.FlushLogs()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> err := command.Execute(); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        _, _ = fmt.Fprintf(os.Stderr, <span class="string">&quot;%v\n&quot;</span>, err)</span><br><span class="line">        os.Exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中 <code>app.WithPlugin(sample.Name, sample.New)</code> 就是我们接下来要实现的插件，从 <code>WithPlugin</code> 函数的参数也可以看出我们这里的 <code>sample.New</code> 必须是一个 <code>framework.PluginFactory</code> 类型的值，而 <code>PluginFactory</code> 的定义就是一个函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span><br></pre></td></tr></table></figure>

<p>所以 <code>sample.New</code> 实际上就是上面的这个函数，在这个函数中我们可以获取到插件中的一些数据然后进行逻辑处理即可，插件实现如下所示，我们这里只是简单获取下数据打印日志，如果你有实际需求的可以根据获取的数据就行处理即可，我们这里只是实现了 <code>PreFilter</code>、<code>Filter</code>、<code>PreBind</code> 三个扩展点，其他的可以用同样的方式来扩展即可：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插件名称</span></span><br><span class="line"><span class="keyword">const</span> Name = <span class="string">&quot;sample-plugin&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Args <span class="keyword">struct</span> &#123;</span><br><span class="line">    FavoriteColor  <span class="keyword">string</span> <span class="string">`json:&quot;favorite_color,omitempty&quot;`</span></span><br><span class="line">    FavoriteNumber <span class="keyword">int</span>    <span class="string">`json:&quot;favorite_number,omitempty&quot;`</span></span><br><span class="line">    ThanksTo       <span class="keyword">string</span> <span class="string">`json:&quot;thanks_to,omitempty&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Sample <span class="keyword">struct</span> &#123;</span><br><span class="line">    args   *Args</span><br><span class="line">    handle framework.FrameworkHandle</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Name</span><span class="params">()</span> <span class="title">string</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> Name</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreFilter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">    klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;prefilter pod: %v&quot;</span>, pod.Name)</span><br><span class="line">    <span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">&quot;&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">Filter</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">    klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;filter pod: %v, node: %v&quot;</span>, pod.Name, nodeName)</span><br><span class="line">    <span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">&quot;&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *Sample)</span> <span class="title">PreBind</span><span class="params">(pc *framework.PluginContext, pod *v1.Pod, nodeName <span class="keyword">string</span>)</span> *<span class="title">framework</span>.<span class="title">Status</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> nodeInfo, ok := s.handle.NodeInfoSnapshot().NodeInfoMap[nodeName]; !ok &#123;</span><br><span class="line">        <span class="keyword">return</span> framework.NewStatus(framework.Error, fmt.Sprintf(<span class="string">&quot;prebind get node info error: %+v&quot;</span>, nodeName))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;prebind node info: %+v&quot;</span>, nodeInfo.Node())</span><br><span class="line">        <span class="keyword">return</span> framework.NewStatus(framework.Success, <span class="string">&quot;&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//type PluginFactory = func(configuration *runtime.Unknown, f FrameworkHandle) (Plugin, error)</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">New</span><span class="params">(configuration *runtime.Unknown, f framework.FrameworkHandle)</span> <span class="params">(framework.Plugin, error)</span></span> &#123;</span><br><span class="line">    args := &amp;Args&#123;&#125;</span><br><span class="line">    <span class="keyword">if</span> err := framework.DecodeInto(configuration, args); err != <span class="literal">nil</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">    &#125;</span><br><span class="line">    klog.V(<span class="number">3</span>).Infof(<span class="string">&quot;get plugin config args: %+v&quot;</span>, args)</span><br><span class="line">    <span class="keyword">return</span> &amp;Sample&#123;</span><br><span class="line">        args: args,</span><br><span class="line">        handle: f,</span><br><span class="line">    &#125;, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>完整代码可以前往仓库 <a target="_blank" rel="noopener" href="https://github.com/cnych/sample-scheduler-framework">https://github.com/cnych/sample-scheduler-framework</a> 获取。</p>
</blockquote>
<p>实现完成后，编译打包成镜像即可，然后我们就可以当成普通的应用用一个 <code>Deployment</code> 控制器来部署即可，由于我们需要去获取集群中的一些资源对象，所以当然需要申请 RBAC 权限，然后同样通过 <code>--config</code> 参数来配置我们的调度器，同样还是使用一个 <code>KubeSchedulerConfiguration</code> 资源对象配置，可以通过 <code>plugins</code> 来启用或者禁用我们实现的插件，也可以通过 <code>pluginConfig</code> 来传递一些参数值给插件：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler-clusterrole</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">endpoints</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">events</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">nodes</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pods</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">delete</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">bindings</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pods/binding</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pods/status</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">patch</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">replicationcontrollers</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">services</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">apps</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">extensions</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">replicasets</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">apps</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">statefulsets</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">policy</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">poddisruptionbudgets</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">persistentvolumeclaims</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">persistentvolumes</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">configmaps</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;storage.k8s.io&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">storageclasses</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">csinodes</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">watch</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;coordination.k8s.io&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">leases</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">get</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">list</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;events.k8s.io&quot;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">events</span></span><br><span class="line">    <span class="attr">verbs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">create</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">patch</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">update</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler-clusterrolebinding</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler-clusterrole</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">scheduler-config.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    apiVersion: kubescheduler.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="string">    kind: KubeSchedulerConfiguration</span></span><br><span class="line"><span class="string">    leaderElection:</span></span><br><span class="line"><span class="string">      leaderElect: true</span></span><br><span class="line"><span class="string">      leaseDuration: 15s</span></span><br><span class="line"><span class="string">      renewDeadline: 10s</span></span><br><span class="line"><span class="string">      resourceLock: endpointsleases</span></span><br><span class="line"><span class="string">      resourceName: sample-scheduler</span></span><br><span class="line"><span class="string">      resourceNamespace: kube-system</span></span><br><span class="line"><span class="string">      retryPeriod: 2s</span></span><br><span class="line"><span class="string">    profiles:</span></span><br><span class="line"><span class="string">      - schedulerName: sample-scheduler</span></span><br><span class="line"><span class="string">        plugins:</span></span><br><span class="line"><span class="string">          preFilter:</span></span><br><span class="line"><span class="string">            enabled:</span></span><br><span class="line"><span class="string">              - name: &quot;sample-plugin&quot;</span></span><br><span class="line"><span class="string">          filter:</span></span><br><span class="line"><span class="string">            enabled:</span></span><br><span class="line"><span class="string">              - name: &quot;sample-plugin&quot;</span></span><br><span class="line"><span class="string">        pluginConfig:</span></span><br><span class="line"><span class="string">          - name: sample-plugin</span></span><br><span class="line"><span class="string">            args:  # runtime.Object</span></span><br><span class="line"><span class="string">              favorColor: &quot;#326CE5&quot;</span></span><br><span class="line"><span class="string">              favorNumber: 7</span></span><br><span class="line"><span class="string">              thanksTo: &quot;Kubernetes&quot;</span></span><br><span class="line"><span class="string"></span><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sample-scheduler</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">sample-scheduler</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">component:</span> <span class="string">sample-scheduler</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">component:</span> <span class="string">sample-scheduler</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">sample-scheduler-sa</span></span><br><span class="line">      <span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br><span class="line">          <span class="attr">configMap:</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">scheduler</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">cnych/sample-scheduler:v0.2.4</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">          <span class="attr">command:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">sample-scheduler</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--config=/etc/kubernetes/scheduler-config.yaml</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--v=3</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/kubernetes</span></span><br><span class="line"><span class="comment">#          livenessProbe:</span></span><br><span class="line"><span class="comment">#            httpGet:</span></span><br><span class="line"><span class="comment">#              path: /healthz</span></span><br><span class="line"><span class="comment">#              port: 10251</span></span><br><span class="line"><span class="comment">#            initialDelaySeconds: 15</span></span><br><span class="line"><span class="comment">#          readinessProbe:</span></span><br><span class="line"><span class="comment">#            httpGet:</span></span><br><span class="line"><span class="comment">#              path: /healthz</span></span><br><span class="line"><span class="comment">#              port: 10251</span></span><br></pre></td></tr></table></figure>

<p>直接部署上面的资源对象即可，这样我们就部署了一个名为 <code>sample-scheduler</code> 的调度器了，接下来我们可以部署一个应用来使用这个调度器进行调度：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-scheduler</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">test-scheduler</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">test-scheduler</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">schedulerName:</span> <span class="string">sample-scheduler</span>  <span class="comment"># 指定使用的调度器，不指定使用默认的default-scheduler</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>这里需要注意的是我们现在手动指定了一个 <code>schedulerName</code> 的字段，将其设置成上面我们自定义的调度器名称 <code>sample-scheduler</code>。</p>
<p>我们直接创建这个资源对象，创建完成后查看我们自定义调度器的日志信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods -n kube-system -l component=sample-scheduler</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">sample-scheduler-896658cd7-k7vcl   1/1     Running   0          57s</span><br><span class="line">➜ kubectl logs -f sample-scheduler-896658cd7-k7vcl -n kube-system</span><br><span class="line">I0114 09:14:18.878613       1 eventhandlers.go:173] add event <span class="keyword">for</span> unscheduled pod default/test-scheduler-6486fd49fc-zjhcx</span><br><span class="line">I0114 09:14:18.878670       1 scheduler.go:464] Attempting to schedule pod: default/test-scheduler-6486fd49fc-zjhcx</span><br><span class="line">I0114 09:14:18.878706       1 sample.go:77] <span class="string">&quot;Start PreFilter Pod&quot;</span> pod=<span class="string">&quot;test-scheduler-6486fd49fc-zjhcx&quot;</span></span><br><span class="line">I0114 09:14:18.878802       1 sample.go:93] <span class="string">&quot;Start Filter Pod&quot;</span> pod=<span class="string">&quot;test-scheduler-6486fd49fc-zjhcx&quot;</span> node=<span class="string">&quot;node2&quot;</span> preFilterState=&amp;&#123;Resource:&#123;MilliCPU:0 Memory:0 EphemeralStorage:0 AllowedPodNumber:0 ScalarResources:map[]&#125;&#125;</span><br><span class="line">I0114 09:14:18.878835       1 sample.go:93] <span class="string">&quot;Start Filter Pod&quot;</span> pod=<span class="string">&quot;test-scheduler-6486fd49fc-zjhcx&quot;</span> node=<span class="string">&quot;node1&quot;</span> preFilterState=&amp;&#123;Resource:&#123;MilliCPU:0 Memory:0 EphemeralStorage:0 AllowedPodNumber:0 ScalarResources:map[]&#125;&#125;</span><br><span class="line">I0114 09:14:18.879043       1 default_binder.go:51] Attempting to <span class="built_in">bind</span> default/test-scheduler-6486fd49fc-zjhcx to node1</span><br><span class="line">I0114 09:14:18.886360       1 scheduler.go:609] <span class="string">&quot;Successfully bound pod to node&quot;</span> pod=<span class="string">&quot;default/test-scheduler-6486fd49fc-zjhcx&quot;</span> node=<span class="string">&quot;node1&quot;</span> evaluatedNodes=3 feasibleNodes=2</span><br><span class="line">I0114 09:14:18.887426       1 eventhandlers.go:205] delete event <span class="keyword">for</span> unscheduled pod default/test-scheduler-6486fd49fc-zjhcx</span><br><span class="line">I0114 09:14:18.887475       1 eventhandlers.go:225] add event <span class="keyword">for</span> scheduled pod default/test-scheduler-6486fd49fc-zjhcx</span><br></pre></td></tr></table></figure>

<p>可以看到当我们创建完 Pod 后，在我们自定义的调度器中就出现了对应的日志，并且在我们定义的扩展点上面都出现了对应的日志，证明我们的示例成功了，也可以通过查看 Pod 的 <code>schedulerName</code> 来验证：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get pods</span><br><span class="line">NAME                              READY   STATUS    RESTARTS       AGE</span><br><span class="line">test-scheduler-6486fd49fc-zjhcx   1/1     Running   0              35s</span><br><span class="line">➜ kubectl get pod test-scheduler-6486fd49fc-zjhcx -o yaml</span><br><span class="line">......</span><br><span class="line">restartPolicy: Always</span><br><span class="line">schedulerName: sample-scheduler</span><br><span class="line">securityContext: &#123;&#125;</span><br><span class="line">serviceAccount: default</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>从 Kubernetes v1.17 版本开始，<code>Scheduler Framework</code> 内置的预选和优选函数已经全部插件化，所以要扩展调度器我们应该掌握并理解调度框架这种方式。</p>
<h2 id="3、调度器调优"><a href="#3、调度器调优" class="headerlink" title="3、调度器调优"></a>3、调度器调优</h2><p>作为 kubernetes 集群的默认调度器，kube-scheduler 主要负责将 Pod 调度到集群的 Node 上。在一个集群中，满足一个 Pod 调度请求的所有节点称之为 <code>可调度 Node</code>，调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 进行打分，之后选出其中得分最高的 Node 来运行 Pod，最后，调度器将这个调度决定告知 <code>kube-apiserver</code>，这个过程叫做<strong>绑定</strong>。</p>
<p>在 Kubernetes 1.12 版本之前，kube-scheduler 会检查集群中所有节点的可调度性，并且给可调度节点打分。Kubernetes 1.12 版本添加了一个新的功能，<strong>允许调度器在找到一定数量的可调度节点之后就停止继续寻找可调度节点</strong>。该功能能提高调度器在大规模集群下的调度性能，这个数值是集群规模的百分比，这个百分比通过 <code>percentageOfNodesToScore</code> 参数来进行配置，其值的范围在 1 到 100 之间，最大值就是 100%，如果设置为 0 就代表没有提供这个参数配置。</p>
<p>Kubernetes 1.14 版本又加入了一个特性，在该参数没有被用户配置的情况下，调度器会根据集群的规模自动设置一个集群比例，然后通过这个比例筛选一定数量的可调度节点进入打分阶段。该特性使用<strong>线性公式</strong>计算出集群比例，比如100个节点的集群下会取 50%，在 5000节点的集群下取 10%，这个自动设置的参数的最低值是 5%，换句话说，<strong>调度器至少会对集群中 5% 的节点进行打分</strong>，除非用户将该参数设置的低于 5。</p>
<p>注意</p>
<blockquote>
<p>当集群中的可调度节点少于 50 个时，调度器仍然会去检查所有节点，因为可调度节点太少，不足以停止调度器最初的过滤选择。如果我们想要关掉这个范围参数，可以将 <code>percentageOfNodesToScore</code> 值设置成 100。</p>
</blockquote>
<p><code>percentageOfNodesToScore</code> 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的，另外 <strong>50</strong> 个 Node 的数值是硬编码在程序里面的，设置这个值的作用在于：<strong>当集群的规模是数百个节点并且 percentageOfNodesToScore 参数设置的过低的时候，调度器筛选到的可调度节点数目基本不会受到该参数影响</strong>。当集群规模较小时，这个设置对调度器性能提升并不明显，但是在超过 1000 个 Node 的集群中，将调优参数设置为一个较低的值可以很明显的提升调度器性能。</p>
<p>不过值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，很多 Node 都没有进入到打分阶段，这样就会造成一种后果，一个本来可以在打分阶段得分很高的 Node 甚至都不能进入打分阶段。由于这个原因，所以这个参数不应该被设置成一个很低的值，<strong>通常的做法是不会将这个参数的值设置的低于 10</strong>，很低的参数值一般在调度器的吞吐量很高且对 Node 的打分不重要的情况下才使用。<strong>换句话说，只有当你更倾向于在可调度节点中任意选择一个 Node 来运行这个 Pod 时，才使用很低的参数设置。</strong></p>
<p><strong>如果你的集群规模只有数百个节点或者更少，实际上并不推荐你将这个参数设置得比默认值更低，因为这种情况下不太会有效的提高调度器性能。</strong></p>
<h2 id="4、创建一个Pod的工作流程"><a href="#4、创建一个Pod的工作流程" class="headerlink" title="4、创建一个Pod的工作流程"></a>4、创建一个Pod的工作流程</h2><p>一般情况下我们部署的 Pod 是通过<strong>集群的自动调度策略</strong>来选择节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均。但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，<strong>比如我们希望一些机器学习的应用只跑在有 GPU 的节点上</strong>；<strong>但是有的时候我们的服务之间交流比较频繁，又希望能够将这服务的 Pod 都调度到同一个的节点上</strong>。这就需要使用一些调度方式来控制 Pod 的调度了，主要有两个概念：<strong>亲和性和反亲和性</strong>，亲和性又分成**节点亲和性(nodeAffinity)和 Pod 亲和性(podAffinity)**。</p>
<h3 id="1-架构图"><a href="#1-架构图" class="headerlink" title="1.架构图"></a>1.架构图</h3><p>Kubernetes基于<code>list-watch机制</code>的控制器架构，实现组件间交互的解耦。<br>其他组件监控自己负责的资源，当这些资源发生变化时，kube-apiserver会通知这些组件，这个过程类似于<code>发布与订阅</code>。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220309130416811.png" alt="image-20220309130416811"></p>
<h3 id="2-剖析过程"><a href="#2-剖析过程" class="headerlink" title="2.剖析过程"></a>2.剖析过程</h3><ul>
<li>我们通过命令行创建一个pod：</li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210614222733041.png" alt="image-20210614222733041"></p>
<ul>
<li>当我们在执行命令<code>kubectl run pod4 --image=nginx</code>后，各组件之间的调用流程是如何的呢？</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1、kubectl向apiserver发送一个创建pod的请求</span><br><span class="line">2、apiserver接收到并向etcd写入存储，写入成功返回一个提示</span><br><span class="line"></span><br><span class="line"><span class="comment">#这个过程类似于老板和顾客之间的关系:</span></span><br><span class="line">	api-server:开店老板</span><br><span class="line">	etcd：仓库</span><br><span class="line">	其他组件：顾客</span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615063729371-1623710250391.png" alt="image-20210615063729371"></p>
<ul>
<li>注意：如果在执行如下<code>kubectl run pod4 --image=nginx</code>命令时，卡着了，说明什么问题呢？</li>
</ul>
<p>–&gt;说明：<br>etcd数据库写入有问题/达到性能瓶颈，或者，api-server和etcd 2者总有一个有问题：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615063918928.png" alt="image-20210615063918928"></p>
<ul>
<li>继续：scheduler向apiserver查询未分配的pod资源，通过自身调度算法选择一个合适的node进行绑定（给这个pod资源打一个标记，标记分配到node1）注意：它这个调度算法还是比较复杂、均匀一点的，它会考虑到你的机器的硬件配置，pod属性等等一些综合的属性；</li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615065028529.png" alt="image-20210615065028529"></p>
<ul>
<li>==问题==：如果scheduler组件有问题，那么此时pod会出现什么状态？</li>
</ul>
<p>答：pod会出现通过<code>kubectl get pod</code>根本看不到你刚创建的pod信息的，更别说它的状态了，因为它根本没分配。</p>
<p>因此，如果你创建的pod信息根本看不到，那么会是哪个组件可能有问题？–&gt;<code>scheduler组件</code>可能出在问题。</p>
<ul>
<li><strong>如果是pending状态：pod是已经绑定到某个节点了</strong>。</li>
</ul>
<ul>
<li>继续流程讲解：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">4、kubelet向apiserver查询分配到自己节点的pod，调用docker api（/var/run/docker.sock）创建容器</span><br><span class="line">5、kubelet获取docker创建容器的状态，并汇报给apiserver，apiserver更新状态到etcd存储</span><br><span class="line">6、kubectl get pods就能查看pod状态</span><br><span class="line"></span><br><span class="line">备注：</span><br><span class="line">kubelet的功能主要是管理容器：</span><br><span class="line"></span><br><span class="line">这个是默认调用的docker api接口</span><br><span class="line">[root@k8s-master ~]<span class="comment">#ll /var/run/docker.sock</span></span><br><span class="line">srw-rw---- 1 root docker 0 Jun 14 11:46 /var/run/docker.sock</span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615070717425.png" alt="image-20210615070717425"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615070800673.png" alt="image-20210615070800673"></p>
<ul>
<li>问题：==controller-manager为什么没用到?==</li>
</ul>
<p>controller-manager是用于管理控制器，例如deployment（rs）、service，因为创建的是一个pod，不受它管理。</p>
<p>如果controller-manager要放在这里，<strong>一般是放在etcd后面的</strong>：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615071819565.png" alt="image-20210615071819565"></p>
<ul>
<li>==kube-proxy为什么没用到？==</li>
</ul>
<p>proxy是用于管理pod网络，例如service，因为没创建service。</p>
<p><strong>kube-proxy的主要功能就是维护好service</strong>，service是k8s的抽象资源；</p>
<ul>
<li>扩展</li>
</ul>
<p>比如，这个容器创建的时候创建失败了，不是一个running状态，也不是一个pending状态。可能就是docker在启动容器时，用你那个镜像启动容器失败了。所以这是你需要用docker去run一个镜像看能不能起来。</p>
<h3 id="3-Pod中影响调度的主要属性"><a href="#3-Pod中影响调度的主要属性" class="headerlink" title="3.Pod中影响调度的主要属性"></a>3.Pod中影响调度的主要属性</h3><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210615073053200.png" alt="image-20210615073053200"></p>
<p>:warning: 注意</p>
<blockquote>
<p>resources: {} 资源调度依据这个，挺重要的；<br>很多大厂都会去二开”schedulerName: default-scheduler”这个调度器的，会去加一些调度策略，进而完成他们的需求；</p>
</blockquote>
<p>🍂 调度原因失败分析</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod &lt;NAME&gt; -o wide</span><br><span class="line"></span><br><span class="line">查看调度失败原因：kubectl describe pod &lt;NAME&gt;</span><br><span class="line">• 节点CPU/内存不足</span><br><span class="line">• 有污点，没容忍 (tolerations)</span><br><span class="line">• 没有匹配到节点标签 （n）</span><br></pre></td></tr></table></figure>

<h3 id="4-调度器需要充分考虑诸多的因素"><a href="#4-调度器需要充分考虑诸多的因素" class="headerlink" title="4.调度器需要充分考虑诸多的因素"></a>4.调度器需要充分考虑诸多的因素</h3><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514221616.png"><br>资源高效利用：装箱率要高！<br>afinity：微服务，分步式系统，网络调用，本机调用，排除了网络调用，额外的传输时间，物理网卡带宽限制！<br>anti-affinity：某个业务的不同副本，不能让其跑在一台机器上，一个机架上，一个地域里，使其分布在不同的故障域。<br>locality：数据本地化，是一个很重要的概念，哪里有数据，我的作业就去哪里，这样可以减少数据拷贝的开销。k8s里的拉取镜像。</p>
<p>🍂</p>
<p>听起来很简单，但这个过程中会涉及<strong>Predicate、Priority等各种调度算法</strong>，还有优先级（Priority ）、<strong>抢占（Preemption）</strong>等各种机制。在实际的调度设计中，有非常多需要考虑的问题，比如：</p>
<blockquote>
<p><strong>1. 公平</strong>：如何保证每个节点都能被分配资源</p>
<p><strong>2. 资源高效利用</strong>：怎样压榨集群的资源能力，让资源被最大化使用</p>
<p><strong>3. 效率</strong>：调度的性能要好，能够尽快地对大批量的 pod 完成调度工作</p>
<p><strong>4. 灵活</strong>：允许用户根据自己的需求控制调度的逻辑</p>
</blockquote>
<h3 id="5-Kubernetes中的资源分配"><a href="#5-Kubernetes中的资源分配" class="headerlink" title="5.Kubernetes中的资源分配"></a>5.Kubernetes中的资源分配</h3><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514230257.png"></p>
<p>1.limits：在Cgroups里使用；cpu.cfs_quota/cpu.cfs_period(10w)=1<br>2.requests：cpu这个requests其实在Cgroup里也起作用。当你多个应用发生资源抢占时，他们抢占的cpu时间比较是多少呢？是通过cpu.share去调节的。k8s是如何实现的呢？这里如果设置的是一个cpu，request是1的话，那么cpu.share是1024。 如果你设置的是100m，相当于是0.1个cpu，那么cpu.share就是0.1*1024=102. 也就是<strong>cpu.requests也是最终会体现到Cgroups里面去的</strong>。</p>
<h3 id="6-Init-container的资源需求"><a href="#6-Init-container的资源需求" class="headerlink" title="6.Init container的资源需求"></a>6.Init container的资源需求</h3><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514230423.png"></p>
<h2 id="5、nodeSelector"><a href="#5、nodeSelector" class="headerlink" title="5、nodeSelector"></a>5、nodeSelector</h2><p>nodeSelector：用于将Pod调度到匹配Label的Node上，<code>如果没有匹配的标签会调度失败</code>。</p>
<p>作用： </p>
<ul>
<li>约束Pod到特定的节点运行</li>
<li>完全匹配节点标签</li>
</ul>
<p>应用场景：</p>
<ul>
<li>专用节点：根据业务线将Node分组管理</li>
<li>配备特殊硬件：部分Node配有SSD硬盘、GPU</li>
</ul>
<table><tr><td bgcolor=yellow>💘 实践：nodeSelector测试(测试成功)-2022.5.16</td></tr></table>

<p>在了解亲和性之前，我们先来了解一个非常常用的调度方式：<code>nodeSelector</code>。我们知道 label 标签是 kubernetes 中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的 Service 对象通过 label 去匹配 Pod 资源，而 <strong>Pod 的调度也可以根据节点的 label 来进行调度</strong>。</p>
<ul>
<li>我们可以通过下面的命令查看我们的 node 的 label：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl get node --show-labels </span></span><br><span class="line">NAME      STATUS   ROLES                  AGE    VERSION   LABELS</span><br><span class="line">master1   Ready    control-plane,master   109d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=</span><br><span class="line">node1     Ready    &lt;none&gt;                 109d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux</span><br><span class="line">node2     Ready    &lt;none&gt;                 109d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux</span><br></pre></td></tr></table></figure>



<ul>
<li>现在我们先给节点 node2 增加一个<code>com=youdianzhishi</code>的标签，命令如下：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl label nodes node2 com=youdianzhishi</span></span><br><span class="line">node/node2 labeled</span><br></pre></td></tr></table></figure>

<p>我们可以通过上面的 <code>--show-labels</code> 参数可以查看上述标签是否生效。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl get node node2 --show-labels </span></span><br><span class="line">NAME    STATUS   ROLES    AGE    VERSION   LABELS</span><br><span class="line">node2   Ready    &lt;none&gt;   109d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux</span><br></pre></td></tr></table></figure>



<ul>
<li>Pod里配置nodeSelector字段：</li>
</ul>
<p>当节点被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 Pod 的 spec 字段中添加 <code>nodeSelector</code> 字段，里面是我们需要被调度的节点的 label 标签，比如，下面的 Pod 我们要强制调度到 node2 这个节点上去，我们就可以使用 nodeSelector 来表示了：</p>
<p>$ vim 01-node-selector-demo.yaml</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 01-node-selector-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">busybox-pod</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-busybox</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;3600&quot;</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-busybox</span></span><br><span class="line">  <span class="attr">nodeSelector:</span> <span class="comment">#注意：nodeSelector是和containers同级的；注意，这个放的顺序一定要放在containers后面。不然会报错的！</span></span><br><span class="line">    <span class="attr">com:</span> <span class="string">youdianzhishi</span></span><br></pre></td></tr></table></figure>



<ul>
<li>部署后，我们就可以通过 describe 命令查看调度结果：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">hg@LAPTOP-G8TUFE0T:/mnt/c/Users/hg/Desktop/yaml$ kubectl apply -f 01-node-selector-demo.yaml </span><br><span class="line">pod/test-busybox created</span><br><span class="line"></span><br><span class="line">hg@LAPTOP-G8TUFE0T:/mnt/c/Users/hg/Desktop/yaml$ kubectl get po  -owide</span><br><span class="line">NAME           READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">test-busybox   1/1     Running   0          78s   10.244.2.210   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@master1 ~]<span class="comment">#kubectl describe po test-busybox  </span></span><br><span class="line">Name:         test-busybox</span><br><span class="line">Namespace:    default</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/172.29.9.53</span><br><span class="line">Start Time:   Thu, 17 Feb 2022 19:45:11 +0800</span><br><span class="line">Labels:       app=busybox-pod</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.244.2.210</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.2.210</span><br><span class="line">Containers:</span><br><span class="line">  test-busybox:</span><br><span class="line">    Container ID:  containerd://1b4d323942e6d305a4ea25f655eaced77f8cb8e4229eaf1972dc9dfb1246a0c0</span><br><span class="line">    Image:         busybox</span><br><span class="line">    Image ID:      docker.io/library/busybox@sha256:5acba83a746c7608ed544dc1533b87c737a0b0fb730301639a0179f9344b1678</span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      sleep</span><br><span class="line">      3600</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 17 Feb 2022 19:45:31 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p5z6t (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  kube-api-access-p5z6t:</span><br><span class="line">    Type:                    Projected (a volume that contains injected data from multiple sources)</span><br><span class="line">    TokenExpirationSeconds:  3607</span><br><span class="line">    ConfigMapName:           kube-root-ca.crt</span><br><span class="line">    ConfigMapOptional:       &lt;nil&gt;</span><br><span class="line">    DownwardAPI:             <span class="literal">true</span></span><br><span class="line">QoS Class:                   BestEffort</span><br><span class="line">Node-Selectors:              com=youdianzhishi</span><br><span class="line">Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists <span class="keyword">for</span> 300s</span><br><span class="line">                             node.kubernetes.io/unreachable:NoExecute op=Exists <span class="keyword">for</span> 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age    From               Message</span><br><span class="line">  ----    ------     ----   ----               -------</span><br><span class="line">  Normal  Scheduled  2m45s  default-scheduler  Successfully assigned default/test-busybox to node2</span><br><span class="line">  Normal  Pulling    2m43s  kubelet            Pulling image <span class="string">&quot;busybox&quot;</span></span><br><span class="line">  Normal  Pulled     2m26s  kubelet            Successfully pulled image <span class="string">&quot;busybox&quot;</span> <span class="keyword">in</span> 17.583571931s</span><br><span class="line">  Normal  Created    2m26s  kubelet            Created container test-busybox</span><br><span class="line">  Normal  Started    2m25s  kubelet            Started container test-busybox</span><br><span class="line">[root@master1 ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>我们可以看到 Events 下面的信息，我们的 Pod 通过默认的 <code>default-scheduler</code> 调度器被绑定到了 node2 节点。不过需要注意的是<code>nodeSelector</code> 属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 <code>Pending</code> 状态。</p>
<p>通过上面的例子我们可以感受到 <code>nodeSelector</code> 的方式比较直观，但是还不够灵活，控制粒度偏大。接下来我们再和大家了解下更加灵活的方式：节点亲和性(nodeAffinity)。</p>
<p>测试结束。😘</p>
<p>🍂 问题：</p>
<p>因pod中nodeSelector里的标签未出现在all node节点，但后续给node打好符合要求的标签，原来处于pending状态的pod会自动迁移过去的吗？–&gt;<code>会的</code>。</p>
<h2 id="6、亲和性和反亲和性调度"><a href="#6、亲和性和反亲和性调度" class="headerlink" title="6、亲和性和反亲和性调度"></a>6、亲和性和反亲和性调度</h2><p>前面我们了解了 kubernetes 调度器的调度流程，我们知道默认的调度器在使用的时候，经过了 <code>predicates</code> 和 <code>priorities</code> 两个阶段，但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 Pod 的调度，这就需要用到 <code>nodeAffinity(节点亲和性)</code>、<code>podAffinity(pod 亲和性)</code> 以及 <code>podAntiAffinity(pod 反亲和性)</code>。</p>
<p>🍂 亲和性调度可以分成<strong>软策略</strong>和<strong>硬策略</strong>两种方式:</p>
<ul>
<li><code>软策略</code>就是如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓</li>
<li><code>硬策略</code>就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然就不干了</li>
</ul>
<p>对于亲和性和反亲和性都有这两种规则可以设置： <code>preferredDuringSchedulingIgnoredDuringExecution</code> 和<code>requiredDuringSchedulingIgnoredDuringExecution</code>，前面的就是软策略，后面的就是硬策略。</p>
<h3 id="1-节点亲和性"><a href="#1-节点亲和性" class="headerlink" title="1.节点亲和性"></a>1.节点亲和性</h3><p>节点亲和性（nodeAffinity）主要是用来控制 Pod 要部署在哪些节点上，以及不能部署在哪些节点上的，<strong>它可以进行一些简单的逻辑组合了</strong>，不只是简单的相等匹配(比如前面的<code>nodeSelector</code>就是标签的=)。</p>
<p>nodeAffinity：节点亲和类似于nodeSelector，可以根据节点上的标签来约束Pod可以调度到哪些节点。</p>
<p>🍂 相比nodeSelector： </p>
<ul>
<li>匹配有更多的逻辑组合，不只是字符串的完全相等，支持的操作符有：<code>In、NotIn、Exists、DoesNotExist、Gt、Lt</code></li>
<li>调度分为软策略和硬策略，而不是硬性要求<ul>
<li>硬（required）：必须满足</li>
<li>软（preferred）：尝试满足，但不保证</li>
</ul>
</li>
</ul>
<p>🍂 这里的匹配逻辑是 label 标签的值在某个列表中，现在 Kubernetes 提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中 (这里的操作符，我们一般只用到in就足够了；)</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<p>:warning: 注意：</p>
<blockquote>
<p>但是需要注意的是如果 <code>nodeSelectorTerms</code> 下面有多个选项的话，满足任何一个条件就可以了；如果 <code>matchExpressions</code>有多个选项的话，则必须同时满足这些条件才能正常调度 Pod。</p>
</blockquote>
<p>🍂</p>
<p>比nodeSelector更高级的一个！<br>matchExpressions比selector更加灵活。</p>
<table><tr><td bgcolor=yellow>💘 实践：节点亲和性测试(测试成功)-2022.5.16</td></tr></table>

<ul>
<li>比如现在我们用一个 Deployment 来管理8个 Pod 副本，现在我们来控制下这些 Pod 的调度，如下例子：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 02-node-affinity-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-affinity</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">node-affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">8</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">node-affinity</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">node-affinity</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginxweb</span></span><br><span class="line">      <span class="attr">affinity:</span> <span class="comment">#定义亲和性</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span> <span class="comment">#节点亲和性 </span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">NotIn</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">master1</span> <span class="comment">#相当于只能调度到node1和node2节点。默认就不会调度到master1节点</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 软策略(尽可能调度到node2节点)</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">            <span class="attr">preference:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">com</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">youdianzhishi</span></span><br></pre></td></tr></table></figure>

<p>上面这个 Pod 首先是要求不能运行在 master1 这个节点上，如果有个节点满足 <code>com=youdianzhishi</code> 的话就优先调度到这个节点上。</p>
<p>由于上面 node02 节点我们打上了 <code>com=youdianzhishi</code> 这样的 label 标签，所以按要求会优先调度到这个节点来的。</p>
<ul>
<li>现在我们来创建这个 Pod，然后查看具体的调度情况是否满足我们的要求。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f node-affinty-demo.yaml</span><br><span class="line">deployment.apps/node-affinity created</span><br><span class="line">➜ kubectl get pods -l app=node-affinity -o wide <span class="comment">#老师这个有部分pod被调度到node1节点</span></span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE     IP             NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">node-affinity-cdd9d54d9-bgbbh   1/1     Running   0          2m28s   10.244.2.247   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-dlbck   1/1     Running   0          2m28s   10.244.4.16    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-g2jr6   1/1     Running   0          2m28s   10.244.4.17    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-gzr58   1/1     Running   0          2m28s   10.244.1.118   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-hcv7r   1/1     Running   0          2m28s   10.244.2.246   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-kvxw4   1/1     Running   0          2m28s   10.244.2.245   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-p4mmk   1/1     Running   0          2m28s   10.244.2.244   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-cdd9d54d9-t5mff   1/1     Running   0          2m28s   10.244.1.117   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>从结果可以看出有5个 Pod 被部署到了 node2 节点上，但是可以看到并没有一个 Pod 被部署到 master1 这个节点上，因为我们的硬策略就是不允许部署到该节点上，而 node2 是软策略，<strong>所以会尽量满足</strong>。</p>
<p>测试结束。😘</p>
<h3 id="2-pod-亲和性和pod-反亲和性"><a href="#2-pod-亲和性和pod-反亲和性" class="headerlink" title="2.pod 亲和性和pod 反亲和性"></a>2.pod 亲和性和pod 反亲和性</h3><p>Pod 亲和性（podAffinity）主要解决 Pod 可以和哪些 Pod 部署在<strong>同一个拓扑域</strong>中的问题（其中<strong>拓扑域用主机标签实现</strong>，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 Pod 反亲和性主要是解决 Pod 不能和哪些 Pod 部署在同一个拓扑域中的问题，它们都是处理的 Pod 与 Pod 之间的关系。比如一个 Pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 Pod 在节点上了，那么我就不想和你待在同一个节点上。</p>
<p>👉🏼 这个是很重要的，线上业务基本要配置这种podAntiAffinity。</p>
<p>🍂 <code>拓扑域</code></p>
<blockquote>
<p>这里要重点理解下什么是拓扑域？–&gt;你可以把它看成为一个分组。</p>
</blockquote>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220218131033136.png" alt="image-20220218131033136"></p>
<table><tr><td bgcolor=yellow>💘 实验：pod亲和性(测试成功)-2022.5.16</td></tr></table>

<ul>
<li>由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 Pod 创建在同一个主机上面。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment"># kubectl get node --show-labels </span></span><br><span class="line">NAME      STATUS   ROLES                  AGE    VERSION   LABELS</span><br><span class="line">master1   Ready    control-plane,master   110d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=</span><br><span class="line">node1     Ready    &lt;none&gt;                 110d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux</span><br><span class="line">node2     Ready    &lt;none&gt;                 110d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux</span><br></pre></td></tr></table></figure>



<ul>
<li>同样，还是针对上面的资源对象，我们来测试下 Pod 的亲和性：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 03-pod-affinity-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-affinity</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod-affinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">pod-affinity</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">pod-affinity</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginxweb</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span> <span class="comment">#去选择具有app in [&quot;busybox-pod&quot;]的pod所在的hostname这个域。</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">busybox-pod</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br></pre></td></tr></table></figure>

<p>上面这个例子中的 Pod 需要调度到某个指定的节点上，并且该节点上运行了一个带有 <code>app=busybox-pod</code> 标签的 Pod。我们可以查看有标签 <code>app=busybox-pod</code> 的 pod 列表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl get pods -l app=busybox-pod -o wide</span></span><br><span class="line">NAME           READY   STATUS    RESTARTS      AGE   IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">test-busybox   1/1     Running   5 (23m ago)   18h   10.244.2.210   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>



<ul>
<li>我们看到这个 Pod 运行在了 node2 的节点上面，所以按照上面的亲和性来说，上面我们部署的3个 Pod 副本也应该运行在 node2 节点上：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 03-pod-affinity-demo.yaml </span><br><span class="line">deployment.apps/pod-affinity created</span><br><span class="line">$ kubectl get pods -o wide -l app=pod-affinity</span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-affinity-785f687c5-52t54   1/1     Running   0          60s   10.244.2.228   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-785f687c5-g594p   1/1     Running   0          60s   10.244.2.229   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-785f687c5-s6j7h   1/1     Running   0          60s   10.244.2.227   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>



<ul>
<li>如果我们把上面的 test-busybox 和 pod-affinity 这个 Deployment 都删除，然后重新创建 pod-affinity 这个资源，看看能不能正常调度呢：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f 01-node-selector-demo.yaml</span><br><span class="line">pod <span class="string">&quot;test-busybox&quot;</span> deleted</span><br><span class="line">$ kubectl delete -f 03-pod-affinity-demo.yaml</span><br><span class="line">deployment.apps <span class="string">&quot;pod-affinity&quot;</span> deleted</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f 03-pod-affinity-demo.yaml </span><br><span class="line">deployment.apps/pod-affinity created</span><br><span class="line">$ kubectl get po</span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod-affinity-785f687c5-2256q   0/1     Pending   0          80s</span><br><span class="line">pod-affinity-785f687c5-7gpz5   0/1     Pending   0          80s</span><br><span class="line">pod-affinity-785f687c5-97gpj   0/1     Pending   0          80s</span><br></pre></td></tr></table></figure>

<p>我们可以看到都处于 <code>Pending</code> 状态了，这是因为现在没有一个节点上面拥有 <code>app=busybox-pod</code> 这个标签的 Pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 Pod 调度到其他节点上，观察下上面的3个副本会不会也被调度到对应的节点上去。(这里可以自己测试下，可以利用node1的<code>kubernetes.io/hostname: node1</code>标签用nodeSelector来实现)</p>
<ul>
<li>我们这个地方使用的是 <code>kubernetes.io/hostname</code> 这个<strong>拓扑域</strong>，意思就是我们当前调度的 Pod 要和目标的 Pod 处于同一个主机上面，因为要处于同一个拓扑域下面。为了说明这个问题，我们把拓扑域改成 <code>beta.kubernetes.io/os</code>，同样的我们当前调度的 Pod 要和目标的 Pod 处于同一个拓扑域中，目标的 Pod 是拥有 <code>beta.kubernetes.io/os=linux</code> 的标签，而我们这里所有节点都有这样的标签，这也就意味着我们所有节点都在同一个拓扑域中，所以我们这里的 Pod 可以被调度到任何一个节点，重新运行上面的 <code>app=busybox-pod</code> 的 Pod，然后再更新下我们这里的资源对象：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get po -owide</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE    IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-affinity-6bf5bb4fc4-j6ctw   1/1     Running   0          22s    10.244.2.230   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-6bf5bb4fc4-xb7tr   1/1     Running   0          22s    10.244.2.231   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-6bf5bb4fc4-xl6pn   1/1     Running   0          22s    10.244.1.97    node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220218145745621.png" alt="image-20220218145745621"></p>
<p>可以看到现在是分别运行在2个节点下面的，因为他们都属于 <code>beta.kubernetes.io/os</code> 这个拓扑域(而<code>busybox-pod</code>也刚好在这个域下，因此符合硬策略要求)。</p>
<blockquote>
<p>:warning: 这里需要注意下：通过上面这个实验可以看到，这2个node节点都属于<code>beta.kubernetes.io/os</code>这个拓扑域，但只有node1上有<code>app=pod-affitity</code>这个标签的pod，从结果可以看到也是可以调度到node2上的。</p>
</blockquote>
<p>实验结束。😘</p>
<p>🍂 pod 反亲和性(podAntiAffinity)</p>
<p>Pod 反亲和性（podAntiAffinity）则是反着来的，比如一个节点上运行了某个 Pod，那么我们的模板 Pod 则不希望被调度到这个节点上面去了。</p>
<table><tr><td bgcolor=yellow>💘 实验：pod反亲和性(测试成功)-2022.5.16</td></tr></table>

<ul>
<li>我们把上面的 <code>podAffinity</code> 直接改成 <code>podAntiAffinity</code>：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 04-pod-antiaffinity-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginxweb</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 硬策略</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span> <span class="comment">#3个pod副本不会调度到具有app=busybox-pod所在的hostanme这个域(节点)上面</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">app</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="string">busybox-pod</span></span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span> <span class="comment">#注意：pod反亲和性是直接不往这个域里直接调度pod的！！！</span></span><br></pre></td></tr></table></figure>

<p>这里的意思就是如果一个节点上面有一个 <code>app=busybox-pod</code> 这样的 Pod 的话，那么我们的 Pod 就别调度到这个节点上面来，上面我们把<code>app=busybox-pod</code> 这个 Pod 固定到了 node2 这个节点上面的，所以正常来说我们这里的 Pod 不会出现在该节点上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 04-pod-antiaffinity-demo.yaml </span><br><span class="line">deployment.apps/pod-antiaffinity created</span><br><span class="line">$ kubectl get po -owide</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-antiaffinity-57c57dd9f7-jspkt   1/1     Running   0          25s   10.244.1.100   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-57c57dd9f7-mm78w   1/1     Running   0          25s   10.244.1.99    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-57c57dd9f7-x9mft   1/1     Running   0          25s   10.244.1.98    node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>我们可以看到没有被调度到 node2 节点上，因为我们这里使用的是 Pod 反亲和性。</p>
<ul>
<li>大家可以思考下，如果这里我们将拓扑域更改成 <code>beta.kubernetes.io/os</code> 会怎么样呢？可以自己去测试下看看。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 04-pod-antiaffinity-demo.yaml </span><br><span class="line">deployment.apps/pod-antiaffinity created</span><br><span class="line">$ kubectl get po</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod-antiaffinity-c5fb4db4d-jj5j7   0/1     Pending   0          4s</span><br><span class="line">pod-antiaffinity-c5fb4db4d-xxnhg   0/1     Pending   0          4s</span><br><span class="line">pod-antiaffinity-c5fb4db4d-zkbgp   0/1     Pending   0          4s</span><br></pre></td></tr></table></figure>

<p>实验结束。😘</p>
<h2 id="7、污点与容忍"><a href="#7、污点与容忍" class="headerlink" title="7、污点与容忍"></a>7、污点与容忍</h2><p>Taint（污点）与Tolerations（污点容忍）</p>
<p><strong>Taints</strong>：避免Pod调度到特定Node上 </p>
<p><strong>Tolerations</strong>：允许Pod调度到持有Taints的Node上</p>
<p>应用场景： </p>
<ul>
<li>专用节点：根据业务线将Node分组管理，希望在默认情况下不调度该节点，只有配置了污点容忍才允许分配</li>
<li>配备特殊硬件：部分Node配有SSD硬盘、GPU，希望在默认情况下不调度该节点，只有配置了污点容忍才允许分配</li>
<li>基于Taint的驱逐</li>
</ul>
<p>对于 <code>nodeAffinity</code> 无论是硬策略还是软策略方式，都是<strong>调度 Pod 到预期节点上</strong>。而污点（Taints）恰好与之相反，<strong>如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 Pod。</strong></p>
<p>比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，<strong>或者把一组具有特殊资源预留给某些 Pod</strong>，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。<strong>我们使用 kubeadm 搭建的集群默认就给 master 节点添加了一个污点标记</strong>，所以我们看到我们平时的 Pod 都没有被调度到 master 上去。</p>
<blockquote>
<p>:warning: 污点：其实是一个label标签，只不过它是一个特殊的label标签。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl get node master1 --show-labels </span></span><br><span class="line">NAME      STATUS   ROLES                  AGE    VERSION   LABELS</span><br><span class="line">master1   Ready    control-plane,master   110d   v1.22.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=(注意，这个是一个空标签),node.kubernetes.io/exclude-from-external-load-balancers=</span><br><span class="line"></span><br><span class="line">[root@master1 ~]<span class="comment">#kubectl describe node master1</span></span><br><span class="line">Name:               master1</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=master1</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">......</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      <span class="literal">false</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>我们可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：<code>node-role.kubernetes.io/master:NoSchedule</code>，就表示master 节点打了一个污点的标记，其中影响的参数是 <code>NoSchedule</code>，表示 Pod 不会被调度到标记为 taints 的节点。除了 <code>NoSchedule</code> 外，还有另外两个选项：</p>
<ul>
<li><strong>PreferNoSchedule</strong>：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去</li>
<li><strong>NoExecute</strong>：该选项意味着**一旦 Taint 生效(被打上taint)**，如该节点内正在运行的 Pod 没有对应容忍（Tolerate）设置，则会直接被逐出。 哈哈😂这个命令也是够狠。。。<code>kubectl taint node k8s-node1 disktype=ssd:NoExecute</code></li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220516220902219.png" alt="image-20220516220902219"></p>
<p>🍂 污点 taint 标记节点的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl taint nodes node2 <span class="built_in">test</span>=node2:NoSchedule</span><br><span class="line">node <span class="string">&quot;node2&quot;</span> tainted</span><br></pre></td></tr></table></figure>

<p>上面的命名将 node2 节点标记为了污点，影响策略是 <code>NoSchedule</code>，**<font color=red>只会影响新的 Pod 调度</font>**，如果仍然希望某个 Pod 调度到 taint 节点上，则必须在 Spec 中做出 Toleration 定义，才能调度到该节点。</p>
<p>🍂 最后如果我们要取消节点的污点标记，可以使用下面的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl taint nodes node2 <span class="built_in">test</span>-</span><br><span class="line">node <span class="string">&quot;node2&quot;</span> untainted</span><br></pre></td></tr></table></figure>



<table><tr><td bgcolor=yellow>💘 实践：污点与容忍(测试成功)-2022.5.16</td></tr></table>

<ul>
<li>比如现在我们想要将一个 Pod 调度到 master 节点：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 05-taint-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">taint</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">taint</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">taint</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">taint</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure>

<p>由于 master 节点被标记为了污点，所以我们这里要想 Pod 能够调度到改节点去，就需要增加容忍的声明：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure>



<ul>
<li>然后创建上面的资源，查看结果：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl apply -f taint-demo.yaml</span><br><span class="line">deployment.apps <span class="string">&quot;taint&quot;</span> created</span><br><span class="line"></span><br><span class="line">➜ kubectl get pods -o wide</span><br><span class="line">NAME                                      READY     STATUS             RESTARTS   AGE       IP             NODE</span><br><span class="line">......</span><br><span class="line">taint-845d8bb4fb-57mhm                    1/1       Running            0          1m        10.244.4.247   node2</span><br><span class="line">taint-845d8bb4fb-bbvmp                    1/1       Running            0          1m        10.244.0.33    master1</span><br><span class="line">taint-845d8bb4fb-zb78x                    1/1       Running            0          1m        10.244.4.246   node2</span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>我们可以看到有一个 Pod 副本被调度到了 master 节点，这就是容忍的使用方法。</p>
<blockquote>
<p>那么问题来了：我这个pod可以调度到没打这个污点的node上去吗？还是说会优先调度到这个打了污点的节点上去呢？。。。</p>
<p>–&gt;经测试：是可以调度上去的。pod配置了容忍之后，原来打了污点的node就可以和其他节点一样去负载pod了，具体调度策略是看调度器的。</p>
</blockquote>
<ul>
<li>最后如果我们要取消节点的污点标记，可以使用下面的命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl taint nodes node2 <span class="built_in">test</span>-</span><br><span class="line">node <span class="string">&quot;node2&quot;</span> untainted</span><br></pre></td></tr></table></figure>

<p>实验结束。😘</p>
<p>🍂 </p>
<p>Taints和Tolerations用于保证Pod 不被调度到不合适的Node上，其中Taint应用于Node上，而Toleration则应用于Pod上。</p>
<p>🍂 nodeSelector/nodeAffinity与Taint/Tolerations区别</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nodeSelector/nodeAffinity--一种主观意识，我要分配在哪个node节点上去；</span><br><span class="line">而Taint/Tolerations：是一种排斥行为，在node上排斥pod分配到自己身上；</span><br><span class="line"></span><br><span class="line">nodeSelector/nodeAffinity--管理员强迫node</span><br><span class="line">taint/toleration--node主动的一脸嫌弃</span><br></pre></td></tr></table></figure>



<p>🍂</p>
<p>k8s的自愈能力/故障转移能力：–&gt;污点和容忍！</p>
<p>🍂  <code>tolerations</code> 属性的写法</p>
<p>对于 <code>tolerations</code> 属性的写法，<strong>其中的 key、value、effect 与 Node 的 Taint 设置需保持一致</strong>， 还有以下几点说明：</p>
<ul>
<li>如果 operator 的值是 <code>Exists</code>，则 value 属性可省略</li>
<li>如果 operator 的值是 <code>Equal</code>，则表示其 key 与 value 之间的关系是 equal(等于)</li>
<li>如果不指定 operator 属性，则默认值为 <code>Equal</code></li>
</ul>
<p>另外，还有两个特殊值：</p>
<ul>
<li><p><strong>空的 key 如果再配合 <code>Exists</code> 就能匹配所有的 key 与 value，也就是是能容忍所有节点的所有 Taints</strong> ！！！😋</p>
</li>
<li><p>空的 effect 匹配所有的 effect</p>
</li>
</ul>
<p>例子：</p>
<p>master节点上，为什么除了静态pod之外，上面还有calico/kube-proxy组件可以运行。</p>
<p>为什么呢？原因在这里：==污点容忍是可以直接放宽条件写的，例如calico，不分什么key什么value，符合带有污点是NoSchedule都可以允许分配。==</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220310074709618.png" alt="image-20220310074709618"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220310075310952.png" alt="image-20220310075310952"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220310075518987.png" alt="image-20220310075518987"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220310075606692.png" alt="image-20220310075606692"></p>
<h2 id="8、nodeName"><a href="#8、nodeName" class="headerlink" title="8、nodeName"></a>8、nodeName</h2><p>nodeName：指定节点名称，用于将Pod调度到指定的Node上，不经过调度器 (nodeName指哪打哪😂,<code>nodeName</code>一般在测试中用，但工作中根本没怎么用到)</p>
<table><tr><td bgcolor=yellow>💘 实践：nodeName测试(测试成功)-2022.5.16</td></tr></table>

<ul>
<li>创建pod的yaml并修改</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#kubectl run pod6 --image=nginx --dry-run=client -o yaml &gt; pod6.yaml</span></span><br><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#vim pod6.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-example</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeName:</span> <span class="string">k8s-node1</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:1.15</span> </span><br></pre></td></tr></table></figure>



<ul>
<li>注意：原来k8s-node1节点是已经打好污点了的。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl describe nodes |grep Taint</span></span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Taints:             ssd=ok:NoSchedule</span><br><span class="line">Taints:             &lt;none&gt;</span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl describe nodes k8s-node1 |grep Taint</span></span><br><span class="line">Taints:             ssd=ok:NoSchedule</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>



<ul>
<li>apply并查看效果</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl apply -f pod6.yaml</span></span><br><span class="line">pod/pod6 created</span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl get pod -o wide</span></span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210618062414511.png" alt="image-20210618062414511"></p>
<ul>
<li>结论</li>
</ul>
<blockquote>
<p>如果在pod里指定了<code>nodeName</code>字段，那么即使该pod没有配置<code>污点容忍</code>，也会被强制开通的指定的节点上去的。</p>
</blockquote>
<p>测试结束。😘</p>
<h2 id="9、优先级调度"><a href="#9、优先级调度" class="headerlink" title="9、优先级调度"></a>9、优先级调度</h2><p>与前面所讲的<strong>调度优选策略中的优先级（Priorities）</strong>不同，前面所讲的优先级指的是节点优先级，而我们这里所说的优先级指的是 <strong>Pod 的优先级</strong>，高优先级的 Pod 会优先被调度，或者在资源不足的情况牺牲低优先级的 Pod，以便于重要的 Pod 能够得到资源部署。</p>
<p>🍂 要定义 Pod 优先级，就需要先定义 <code>PriorityClass</code> 对象，该对象没有 Namespace 的限制：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">high-priority</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;This priority class should be used for XYZ service pods only.&quot;</span></span><br></pre></td></tr></table></figure>

<p>其中：</p>
<ul>
<li>value 为 32 位整数的优先级，该值越大，优先级越高</li>
<li>globalDefault 用于未配置 PriorityClassName 的 Pod，整个集群中应该只有一个 <code>PriorityClass</code> 将其设置为 true</li>
</ul>
<p>🍂 然后通过在 Pod 的 <code>spec.priorityClassName</code> 中指定已定义的 <code>PriorityClass</code> 名称即可：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">priorityClassName:</span> <span class="string">high-priority</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>另外一个值得注意的是当节点没有足够的资源供调度器调度 Pod，导致 Pod 处于 pending 时，<strong>抢占（preemption）逻辑就会被触发</strong>，抢占会尝试从一个节点删除低优先级的 Pod，从而释放资源使高优先级的 Pod 得到节点资源进行部署。</p>
</blockquote>
<h2 id="10、QOS"><a href="#10、QOS" class="headerlink" title="10、QOS"></a>10、QOS</h2><h3 id="1-服务质量"><a href="#1-服务质量" class="headerlink" title="1.服务质量"></a>1.服务质量</h3><p><code>QoS</code> 是 <code>Quality of Service</code> 的缩写，即服务质量，为了实现资源被有效调度和分配的同时提高资源利用率，Kubernetes 针对不同服务质量的预期，通过 QoS 来对 pod 进行服务质量管理。对于一个 pod 来说，服务质量体现在两个具体的指标：CPU 和内存。当节点上内存资源紧张时，Kubernetes 会根据预先设置的不同 QoS 类别进行相应处理。</p>
<h3 id="2-资源限制"><a href="#2-资源限制" class="headerlink" title="2.资源限制"></a>2.资源限制</h3><p>如果未做过节点 nodeSelector、亲和性（node affinity）或 pod 亲和、反亲和性等高级调度策略设置，我们没有办法指定服务部署到指定节点上，这样就可能会造成 CPU 或内存等<strong>密集型的 pod</strong> 同时分配到相同节点上，<strong>造成资源竞争</strong>。另一方面，如果未对资源进行限制，一些关键的服务可能会因为资源竞争因 <strong>OOM</strong> 等原因被 kill 掉，或者被限制 CPU 使用。</p>
<p>我们知道对于每一个资源，container 可以指定具体的资源需求（requests）和限制（limits），<strong>requests 申请范围是0到节点的最大配置，而 limits 申请范围是 requests 到无限</strong>，即 <code>0 &lt;= requests &lt;= Node Allocatable</code>, <code>requests &lt;= limits &lt;= Infinity</code>。</p>
<p>对于 CPU，如果 pod 中服务使用的 CPU 超过设置的 limits，<strong>pod 不会被 kill 掉但会被限制</strong>，因为 CPU 是可压缩资源，如果没有设置 limits，pod 可以使用全部空闲的 CPU 资源。</p>
<p>对于内存，当一个 pod 使用内存超过了设置的 limits，<strong>pod 中容器的进程会被 kernel 因 OOM kill 掉</strong>，当 container 因为 OOM 被 kill 掉时，<strong>系统倾向于在其原所在的机器上重启该 container 或本机或其他重新创建一个 pod</strong>。(包括我们的磁盘也是一个<code>不可压缩资源</code>)</p>
<p>🍂 关键是你作为运维，你要大概清楚每个服务消耗资源的情况</p>
<p>我们知道，<strong>tomcat一般是比较耗费资源的，起码需要1c以上</strong>；(java应用特别消耗cpu资源、内存资源)</p>
<p>如果你的pod里的requests配置为0.5c，而limits就配置为了0.8c，那么有可能你的容器都启动不起来；</p>
<p><strong>不像nginx，你给它个100m，200m，它都能起来</strong>。</p>
<p>🍂 主机cpu资源利用率满时，机器出现的现象–&gt;==卡顿==。</p>
<p>问题：默认情况下，宿主机和容器都没做资源限制的话，容器理论上是可以无限制获取到主机的计算资源(如cpu和内存)。如果某个容器应用特别耗cpu，当它把主机cpu耗尽时(资源利用率超高，达到98,99%以上)，问主机会出现什么现象？</p>
<p>答：会出现卡顿现象，执行一条命令，老半天才出来，像老年痴呆一样；</p>
<p>是因为cpu是按时间片来分配的，<strong>cpu的分配原则：为了尽可能地公平地给大家一个分配，但cpu没有特别硬性的限制</strong>；</p>
<p>🍂 总结</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、宿主机只有2c，这里的limits可以设置为100c，但是毫无意义，只是可以这样设置。limits是最大可用资源，一般是requests的20%左右，不太超出太多，否则limits限制就没多少意义了;</span><br><span class="line">2、limits不能小于requests;</span><br><span class="line">3、reqeusts只是一个预留性质，不是pod配置写多少，宿主机就会占用多少资源;</span><br><span class="line">4、k8s根据request来统计每个节点预分配资源，来判断下一个pod能不能分配我这个节点;</span><br><span class="line">5、所以，我们工作中，一定要配置requests和limits，来解决资源争抢问题；</span><br><span class="line">6、limits不配置的话，默认和requests配置一样；</span><br></pre></td></tr></table></figure>

<h3 id="3-QoS-分类"><a href="#3-QoS-分类" class="headerlink" title="3.QoS 分类"></a>3.QoS 分类</h3><p><strong>Kubelet 提供 QoS 服务质量管理，支持系统级别的 OOM 控制。</strong>在 Kubernetes 中，QoS 主要分为 <code>Guaranteed</code>、<code>Burstable</code> 和 <code>Best-Effort</code> 三类，优先级从高到低。</p>
<p>QoS 分类并不是通过一个配置项来直接配置的，而是通过配置 CPU/内存的 limits 与 requests 值的大小来确认服务质量等级的，我们通过使用 <code>kubectl get pod xxx -o yaml</code> 可以看到 pod 的配置输出中有 <code>qosClass</code> 一项，<strong>该配置的作用是为了给资源调度提供策略支持</strong>，调度算法根据不同的服务质量等级可以确定将 pod 调度到哪些节点上。</p>
<h4 id="1、Guaranteed-有保证的"><a href="#1、Guaranteed-有保证的" class="headerlink" title="1、Guaranteed(有保证的)"></a>1、Guaranteed(有保证的)</h4><p>系统用完了全部内存，且没有其他类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉，也就是说最后才会被考虑 kill 掉，属于该级别的 pod 有以下两种情况：</p>
<ul>
<li>pod 中的所有容器都且仅设置了 CPU 和内存的 limits</li>
<li>pod 中的所有容器都设置了 CPU 和内存的 requests 和 limits ，且单个容器内的 <code>requests==limits</code>（requests不等于0）</li>
</ul>
<p>1️⃣ pod 中的所有容器都且仅设置了 limits：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">100Mi</span></span><br></pre></td></tr></table></figure>

<p>因为如果一个容器只指明 limit 而未设定 requests，<strong>则 requests 的值等于 limit 值</strong>，所以上面 pod 的 QoS 级别属于 Guaranteed。</p>
<blockquote>
<p>:warning: 另外需要注意<strong>若容器指定了 requests 而未指定 limits，则 limits 的值等于节点资源的最大值；若容器指定了 limits 而未指定 requests，则 requests 的值等于 limits。</strong></p>
</blockquote>
<p>2️⃣ 另外一个就是 pod 中的所有容器都明确设置了 requests 和 limits，且单个容器内的 <code>requests==limits</code>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">100Mi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">100Mi</span></span><br></pre></td></tr></table></figure>

<p>容器 foo 和 bar 内 resources 的 requests 和 limits 均相等，该 pod 的 QoS 级别属于 Guaranteed。</p>
<p>🍂 CPU单位换算</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">容器资源限制：</span><br><span class="line">• resources.limits.cpu</span><br><span class="line">• resources.limits.memory</span><br><span class="line"></span><br><span class="line">容器使用的最小资源需求，作为容器调度时资源分配的依据：</span><br><span class="line">• resources.requests.cpu</span><br><span class="line">• resources.requests.memory</span><br><span class="line"></span><br><span class="line">CPU单位：可以写m也可以写浮点数，例如0.5=500m，1=1000m</span><br><span class="line">CPU单位：m 毫核，1核=1000m=1，2核=2000m=2，0.5核=500m=0.5，0.2核=200m=0.2</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span> <span class="comment">#特别注意：内存的单位是Mi,cpu的单位是m</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">        <span class="attr">limits:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span> </span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br><span class="line"><span class="comment">#K8s会根据Request的值去查找有足够资源的Node来调度此Pod</span></span><br></pre></td></tr></table></figure>

<h4 id="2、Burstable-不稳定的"><a href="#2、Burstable-不稳定的" class="headerlink" title="2、Burstable(不稳定的)"></a>2、Burstable(不稳定的)</h4><p>系统用完了全部内存，且没有 Best-Effort 类型的容器可以被 kill 时，该类型的 pods 会被 kill 掉。pod 中只要有一个容器的 requests 和 limits 的设置不相同，该 pod 的 QoS 即为 Burstable。</p>
<p>1️⃣ 比如容器 foo 指定了 resource，而容器 bar 未指定：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">10m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bar</span></span><br></pre></td></tr></table></figure>



<p>2️⃣ 或者容器 foo 设置了内存 limits，而容器 bar 设置了 CPU limits：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">100m</span></span><br></pre></td></tr></table></figure>

<p>上面两种情况定义的 pod 都属于 Burstable 类别的 QoS。</p>
<h4 id="3、Best-Effort-尽最大努力"><a href="#3、Best-Effort-尽最大努力" class="headerlink" title="3、Best-Effort(尽最大努力)"></a>3、Best-Effort(尽最大努力)</h4><p>系统用完了全部内存时，该类型 pods 会最先被 kill 掉。<strong>如果 pod 中所有容器的 resources 均未设置 requests 与 limits</strong>，那么该 pod 的 QoS 即为 Best-Effort。</p>
<p>1️⃣ 比如容器 foo 和容器 bar 均未设置 requests 和 limits：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">foo</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bar</span></span><br><span class="line">    <span class="attr">resources:</span></span><br></pre></td></tr></table></figure>

<h4 id="资源限制对Pod调度的影响"><a href="#资源限制对Pod调度的影响" class="headerlink" title="资源限制对Pod调度的影响"></a>资源限制对Pod调度的影响</h4><table><tr><td bgcolor=yellow>💘 实践：resources.requests测试(测试成功)-2022.5.17</td></tr></table>

<ul>
<li>创建yaml并修改</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#kubectl run pod6 --image=nginx --dry-run=client -o yaml &gt; resources.yaml</span></span><br><span class="line"><span class="string">root@k8s-master</span> <span class="string">~]#vim</span> <span class="string">resources.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">pod6</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod6</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">pod6</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment">#最小资源</span></span><br><span class="line">      <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">      <span class="attr">memory:</span> <span class="string">512Mi</span></span><br></pre></td></tr></table></figure>



<ul>
<li>部署并查看</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl apply  -f resources.yaml</span></span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl get pod</span></span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616063348641.png" alt="image-20210616063348641"></p>
<ul>
<li>拷贝刚才创建的那个yaml文件并修改配置</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#cp resources.yaml resources2.yaml</span></span><br><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#vim resources2.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">resource-2</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resource-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">resource-2</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment">#最小资源</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">1500m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">512m</span> <span class="comment">#这里的单位写错了，应该是Mi才对的。。。</span></span><br></pre></td></tr></table></figure>



<ul>
<li>部署并查看，resource-2也起来了：</li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616063948332.png" alt="image-20210616063948332"></p>
<ul>
<li>再次拷贝刚才创建的那个yaml文件并修改配置</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#cp resources2.yaml resources3.yaml</span></span><br><span class="line">[root@k8s-master ~]<span class="comment">#vim resources3.yaml</span></span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl apply -f resources3.yaml</span></span><br><span class="line">pod/resource-3 created</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616064233338.png" alt="image-20210616064233338"></p>
<p>部署并查看效果：</p>
<p>此时发现最后创建的那个pod 一直是pending状态：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616064551464.png" alt="image-20210616064551464"></p>
<ul>
<li>为什么呢？我们查看一下它的描述：</li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616064727484.png" alt="image-20210616064727484"></p>
<p>==<strong>是因为node节点的cpu不足，导致pod调度失败的</strong>；==</p>
<p>sufficient adj.充足的，足够的  insufficient adj.不充足的，不足够的</p>
<ul>
<li>自己机器计算配置大小：</li>
</ul>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616072559721.png" alt="image-20210616072559721"></p>
<p>查看宿主机的资源使用率情况：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl describe node  k8s-node1</span></span><br></pre></td></tr></table></figure>

<p>可分配的：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616072242236.png" alt="image-20210616072242236"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616072411357.png" alt="image-20210616072411357"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616073215481.png" alt="image-20210616073215481"></p>
<ul>
<li>此时，node1可用cpu容量为：8%m  node2可用总量为：63%m </li>
</ul>
<p><strong>而此时resource3.yaml里的pod需求为1900m，很明显以上2个node均无法满足其需求，因此和这个pod将无法被分配，一直处于pending状态</strong>。</p>
<ul>
<li>总结</li>
</ul>
<p><strong>K8s会根据Request的值去查找有足够资源的Node来调度此Pod</strong>，这边看的就是这里的值。</p>
<p><strong>默认你不配置这个requests的值，这里resuest值就是0，即这个pod可以完全使用宿主机所有的cpu 资源，各个pod可能会出现争抢cpu现象</strong>。</p>
<p>实验结束。😘</p>
<table><tr><td bgcolor=yellow>💘 实践：resources.limits测试(测试成功)-2022.5.17</td></tr></table>

<ul>
<li>创建yaml并修改</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#cp resources3.yaml resources4.yaml</span></span><br><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#vim resources4.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">resource-4</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resource-4</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">resource-4</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment">#最小资源</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">      <span class="comment">#最大资源，一般是requests 20%左右</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">600m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">612Mi</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>最大资源，一般是requests 20%左右</p>
</blockquote>
<ul>
<li>apply下并查看</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl apply -f resources4.yaml #这个是不会用影响调度的</span></span><br><span class="line">pod/resource-4 created</span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl get pod</span></span><br><span class="line">NAME                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod4                  1/1     Running   0          33h</span><br><span class="line">pod6                  1/1     Running   0          81m</span><br><span class="line">resource-2            1/1     Running   0          75m</span><br><span class="line">resource-3            0/1     Pending   0          70m</span><br><span class="line">resource-4            1/1     Running   0          6s <span class="comment">#已成功启动pod</span></span><br><span class="line">web-96d5df5c8-wwc96   1/1     Running   0          41h</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>



<ul>
<li>pod 的最大使用上线是可以查看的</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl describe pod resource-4</span></span><br></pre></td></tr></table></figure>

<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20210616075703778.png" alt="image-20210616075703778"></p>
<ul>
<li>我们再次测试下：把limits对应值改大，会不会影响pod开通？=&gt;不会。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#cp resources4.yaml resources5.yaml</span></span><br><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#vim resources5.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">resource-5</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resource-5</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">resource-5</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment">#最小资源</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">      <span class="comment">#最大资源，一般是requests 20%左右</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">1900m</span> <span class="comment">#把这个改成1900m,看会不会影响它开通</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">612Mi</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment">#kubectl apply -f resources5.yaml</span></span><br><span class="line">pod/resource-5 created</span><br><span class="line">[root@k8s-master ~]<span class="comment">#kubectl get pod</span></span><br><span class="line">NAME                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod4                  1/1     Running   0          33h</span><br><span class="line">pod6                  1/1     Running   0          88m</span><br><span class="line">resource-2            1/1     Running   0          81m</span><br><span class="line">resource-3            0/1     Pending   0          77m</span><br><span class="line">resource-4            1/1     Running   0          6m45s</span><br><span class="line">resource-5            1/1     Running   0          6s <span class="comment">#可正常</span></span><br><span class="line">web-96d5df5c8-wwc96   1/1     Running   0          41h</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p>是不会受影响的，就可以正常开通。</p>
<ul>
<li>因此，最终yaml文件标准输出：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">root@k8s-master</span> <span class="string">~</span>]<span class="comment">#vim resources5.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">run:</span> <span class="string">resource-5</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resource-5</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">resource-5</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="comment">#最小资源</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">      <span class="comment">#最大资源，一般是requests 20%左右</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">600m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">612Mi</span></span><br></pre></td></tr></table></figure>

<p>实验结束。😘</p>
<h3 id="4-QoS-解析"><a href="#4-QoS-解析" class="headerlink" title="4.QoS 解析"></a>4.QoS 解析</h3><p><strong>首先我们要明确在调度时调度器只会根据 requests 值进行调度。</strong>当系统 OOM 上时对于处理不同 OOMScore 的进程表现不同，<strong>OOMScore</strong> 是针对 memory 的，<strong>当宿主上 memory 不足时系统会优先 kill 掉 OOMScore 值高的进程</strong>，可以使用 <code>cat /proc/$PID/oom_score</code> 命令查看进程的 OOMScore。OOMScore 的取值范围为 <code>[-1000, 1000]</code>，Guaranteed 类型的 pod 的默认值为 -998，Burstable pod 的值为 <code>2~999</code>，BestEffort pod 的值为 1000。也就是说当系统 OOM 时，首先会 kill 掉 BestEffort pod 的进程，若系统依然处于 OOM 状态，然后才会 kill 掉 Burstable pod，最后是 Guaranteed pod。</p>
<p>Kubernetes 是通过 cgroup 给 pod 设置 QoS 级别的，kubelet 中有一个 <code>--cgroups-per-qos</code> 参数（默认启用），启用后 kubelet 会为不同 QoS 创建对应的 level cgroups，在 Qos level cgroups 下也会为 pod 下的容器创建对应的 level cgroups，从 <code>Qos –&gt; pod –&gt; container</code>，层层限制每个 level cgroups 的资源使用量。</p>
<p>由于我们这里使用的是 containerd 这种容器运行时，则 cgroup 的路径与之前的 docker 不太一样：</p>
<ul>
<li>Guaranteed 类型的 cgroup level 会直接创建在 <code>RootCgroup/system.slice/containerd.service/kubepods-pod&lt;uid&gt;.slice:cri-containerd:&lt;container-id&gt;</code> 下</li>
<li>Burstable 的创建在 <code>RootCgroup/system.slice/containerd.service/kubepods-burstable-pod&lt;uid&gt;.slice:cri-containerd:&lt;container-id&gt;</code> 下</li>
<li>BestEffort 类型的创建在 <code>RootCgroup/system.slice/containerd.service/kubepods-besteffort-pod&lt;uid&gt;.slice:cri-containerd:&lt;container-id&gt;</code> 下</li>
</ul>
<p>:warning: 这里的路径和底层容器运行时及是使用<code>systemd</code>还是<code>cgroupfs</code>都是不一样的。</p>
<blockquote>
<p>注意：我下面的实验路径和老师的是有出入来着，但是总体问题不大；<br>一个pause容器+主容器；</p>
</blockquote>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220220102223206.png" alt="image-20220220102223206"></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220220152022806.png" alt="image-20220220152022806"></p>
<table><tr><td bgcolor=yellow>💘 实践：Qos解析(测试成功)-2022.5.17</td></tr></table>

<ul>
<li>我们可以通过 <code>mount | grep cgroup</code> 命令查看 RootCgroup：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ mount | grep cgroup</span><br><span class="line">tmpfs on /sys/fs/cgroup <span class="built_in">type</span> tmpfs (ro,nosuid,nodev,noexec,mode=755)</span><br><span class="line">cgroup on /sys/fs/cgroup/systemd <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer <span class="built_in">type</span> cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br></pre></td></tr></table></figure>



<p>在 cgroup 的每个子系统下都会创建 QoS level cgroups， 此外在对应的 QoS level cgroups 还会为 pod 创建 Pod level cgroups。</p>
<ul>
<li>比如我们创建一个如下所示的 Pod：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 01-qos-demo.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">qos-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:latest</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">250m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">2Gi</span></span><br></pre></td></tr></table></figure>

<p>直接创建上面的资源对象即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 01-qos-demo.yaml </span><br><span class="line">pod/qos-demo created</span><br><span class="line">$ kubectl get po qos-demo -owide</span><br><span class="line">NAME       READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">qos-demo   1/1     Running   0          27s   10.244.1.241   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">$ kubectl get po qos-demo -owide -oyaml|grep qosClass</span><br><span class="line">  qosClass: Burstable</span><br><span class="line">$ kubectl get po qos-demo -owide -oyaml|grep uid</span><br><span class="line">  uid: 562d098f-8346-4e8f-b23d-9d8c24b75b6a</span><br></pre></td></tr></table></figure>

<p>由于该 pod 的设置的资源 requests != limits，所以其属于 Burstable 类别的 pod。</p>
<p>kubelet 会在其所属 QoS 下创建 <code>RootCgroup/system.slice/containerd.service/kubepods-burstable-pod&lt;uid&gt;.slice:cri-containerd:&lt;container-id&gt;</code> 这个 cgroup level，比如我们查看内存这个子系统的 cgroup：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 还有一个 pause 容器的 cgroup level</span></span><br><span class="line">➜ ls /sys/fs/cgroup/memory/system.slice/containerd.service/kubepods-burstable-pod489a19f2_8d75_474c_988f_5854b61b839f.slice:cri-containerd:4782243ba3260125513af20689fcea31b52eae1cbabeafeb1f7a52bcdcd5b44b</span><br><span class="line">cgroup.clone_children           memory.kmem.tcp.max_usage_in_bytes  memory.oom_control</span><br><span class="line">cgroup.event_control            memory.kmem.tcp.usage_in_bytes      memory.pressure_level</span><br><span class="line">cgroup.procs                    memory.kmem.usage_in_bytes          memory.soft_limit_in_bytes</span><br><span class="line">memory.failcnt                  memory.limit_in_bytes               memory.stat</span><br><span class="line">memory.force_empty              memory.max_usage_in_bytes           memory.swappiness</span><br><span class="line">memory.kmem.failcnt             memory.memsw.failcnt                memory.usage_in_bytes</span><br><span class="line">memory.kmem.limit_in_bytes      memory.memsw.limit_in_bytes         memory.use_hierarchy</span><br><span class="line">memory.kmem.max_usage_in_bytes  memory.memsw.max_usage_in_bytes     notify_on_release</span><br><span class="line">memory.kmem.slabinfo            memory.memsw.usage_in_bytes         tasks</span><br><span class="line">memory.kmem.tcp.failcnt         memory.move_charge_at_immigrate</span><br><span class="line">memory.kmem.tcp.limit_in_bytes  memory.numa_stat</span><br></pre></td></tr></table></figure>

<p>上面创建的应用容器进程 ID 会被写入到上面的 tasks 文件中：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ cat tasks</span><br><span class="line">64133</span><br><span class="line">64170</span><br><span class="line">64171</span><br><span class="line">64172</span><br><span class="line">64173</span><br><span class="line">➜ ps -aux |grep nginx</span><br><span class="line">root      64133  0.0  0.0   8840  3488 ?        Ss   15:56   0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">101       64170  0.0  0.0   9228  1532 ?        S    15:56   0:00 nginx: worker process</span><br><span class="line">101       64171  0.0  0.0   9228  1532 ?        S    15:56   0:00 nginx: worker process</span><br><span class="line">101       64172  0.0  0.0   9228  1532 ?        S    15:56   0:00 nginx: worker process</span><br><span class="line">101       64173  0.0  0.0   9228  1532 ?        S    15:56   0:00 nginx: worker process</span><br></pre></td></tr></table></figure>

<p>这样我们的容器进程就会受到该 cgroup 的限制了，在 pod 的资源清单中我们设置了 memory 的 limits 值为 2Gi，kubelet 则会将该限制值写入到 <code>memory.limit_in_bytes</code> 中去：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ cat memory.limit_in_bytes</span><br><span class="line">2147483648 <span class="comment"># 2147483648 / 1024 / 1024 / 1024 = 2</span></span><br></pre></td></tr></table></figure>



<ul>
<li>同样对于 cpu 资源一样可以在对应的子系统中找到创建的对应 cgroup：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜ ls /sys/fs/cgroup/cpu/system.slice/containerd.service/kubepods-burstable-pod489a19f2_8d75_474c_988f_5854b61b839f.slice:cri-containerd:4782243ba3260125513af20689fcea31b52eae1cbabeafeb1f7a52bcdcd5b44b</span><br><span class="line">cgroup.clone_children  cpuacct.stat          cpu.cfs_period_us  cpu.rt_runtime_us  notify_on_release</span><br><span class="line">cgroup.event_control   cpuacct.usage         cpu.cfs_quota_us   cpu.shares         tasks</span><br><span class="line">cgroup.procs           cpuacct.usage_percpu  cpu.rt_period_us   cpu.stat</span><br><span class="line">➜ cat tasks</span><br><span class="line">64133</span><br><span class="line">64170</span><br><span class="line">64171</span><br><span class="line">64172</span><br><span class="line">64173</span><br><span class="line">➜ cat cpu.cfs_quota_us</span><br><span class="line">50000  <span class="comment"># 500m</span></span><br></pre></td></tr></table></figure>



<p>:warning: 最后关于 QoS 还有一点建议:</p>
<blockquote>
<p>如果资源充足，可将 QoS pods 类型均设置为 Guaranteed。<strong>用计算资源换业务性能和稳定性</strong>，减少排查问题时间和成本。</p>
<p>如果想更好的提高资源利用率，业务服务可以设置为 Guaranteed，而其他服务根据重要程度可分别设置为 Burstable 或 Best-Effort。</p>
</blockquote>
<p>实验结束。😘</p>
<h2 id="11、拓扑分布约束"><a href="#11、拓扑分布约束" class="headerlink" title="11、拓扑分布约束"></a>11、拓扑分布约束</h2><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220218152232851.png" alt="image-20220218152232851"></p>
<h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><blockquote>
<p>:warning:<br>这个理解起来稍微有些复杂；<br>这个可能用的不多，但如果你想进行一些细粒度的控制的话，用这个还是很不错的；</p>
</blockquote>
<p>在 k8s 集群调度中，<strong>亲和性</strong>相关的概念本质上都是控制 Pod 如何被调度 – <strong>堆叠或打散</strong>。</p>
<p><code>podAffinity</code> 以及 <code>podAntiAffinity</code> 两个特性对 Pod 在不同拓扑域的分布进行了一些控制。</p>
<p><code>podAffinity</code> 可以将无数个 Pod 调度到特定的某一个拓扑域，这是<strong>堆叠</strong>的体现；</p>
<p><code>podAntiAffinity</code> 则可以控制一个拓扑域只存在一个 Pod，这是<strong>打散</strong>的体现；</p>
<p>但这两种情况都太极端了，在不少场景下都无法达到理想的效果，<strong>例如为了实现容灾和高可用，将业务 Pod 尽可能均匀的分布在不同可用区就很难实现。</strong></p>
<p><code>PodTopologySpread（Pod 拓扑分布约束）</code> 特性的提出正是为了对 Pod 的调度分布提供更精细的控制，以提高服务可用性以及资源利用率。<code>PodTopologySpread</code> 由 <code>EvenPodsSpread</code> <strong>特性门</strong>所控制，在 v1.16 版本第一次发布，并在 v1.18 版本进入 beta 阶段默认启用。</p>
<h3 id="2-使用规范"><a href="#2-使用规范" class="headerlink" title="2.使用规范"></a>2.使用规范</h3><p>在 Pod 的 Spec 规范中新增了一个 <code>topologySpreadConstraints</code> 字段即可配置拓扑分布约束，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="string">&lt;integer&gt;</span> <span class="comment">#这个属性理解起来不是那么地直接。。。；倾斜度；</span></span><br><span class="line">    <span class="attr">topologyKey:</span> <span class="string">&lt;string&gt;</span></span><br><span class="line">    <span class="attr">whenUnsatisfiable:</span> <span class="string">&lt;string&gt;</span></span><br><span class="line">    <span class="attr">labelSelector:</span> <span class="string">&lt;object&gt;</span></span><br></pre></td></tr></table></figure>

<p>由于这个新增的字段是在 Pod spec 层面添加，因此更高层级的控制 (Deployment、DaemonSet、StatefulSet) 也能使用 <code>PodTopologySpread</code> 功能。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325152221.png" alt="Pod拓扑分布约束"></p>
<p>让我们结合上图来理解 <code>topologySpreadConstraints</code> 中各个字段的含义和作用：</p>
<ul>
<li><code>labelSelector</code>: 用来查找匹配的 Pod，我们能够计算出每个拓扑域中匹配该 label selector 的 Pod 数量，在上图中，假如 label selector 是 <code>app:foo</code>，那么 zone1 的匹配个数为 2， zone2 的匹配个数为 0。</li>
<li><code>topologyKey</code>: 是 Node label 的 key，如果两个 Node 的 label 同时具有该 key 并且值相同，就说它们在同一个拓扑域。在上图中，指定 <code>topologyKey</code> 为 zone， 则具有 <code>zone=zone1</code> 标签的 Node 被分在一个拓扑域，具有 <code>zone=zone2</code> 标签的 Node 被分在另一个拓扑域。</li>
<li>:warning: <code>maxSkew</code>: 这个属性理解起来不是很直接，<strong>它描述了 Pod在不同拓扑域中不均匀分布的最大程度</strong>（指定拓扑类型中任意两个拓扑域中匹配的 Pod 之间的最大允许差值），<strong>它必须大于零</strong>。<strong>每个拓扑域都有一个 skew 值</strong>，计算的公式是:<code>skew[i] = 拓扑域[i]中匹配的 Pod 个数 - min&#123;其他拓扑域中匹配的 Pod 个数&#125;</code>。在上图中，我们新建一个带有<code>app=foo</code>标签的 Pod：<ul>
<li>如果该 Pod 被调度到 zone1，那么 zone1 中 Node 的 skew 值变为 3，zone2 中 Node 的 skew 值变为 0 (zone1 有 3 个匹配的 Pod，zone2 有 0 个匹配的 Pod )</li>
<li>如果该 Pod 被调度到 zone2，那么 zone1 中 Node 的 skew 值变为 2，zone2 中 Node 的 skew 值变为 1(zone2 有 1 个匹配的 Pod，拥有全局最小匹配 Pod 数的拓扑域正是 zone2 自己 )，则它满足<code>maxSkew: 1</code> 的约束（差值为1）</li>
</ul>
</li>
<li><code>whenUnsatisfiable</code>: 描述了如果 Pod 不满足分布约束条件该采取何种策略：<ul>
<li><strong>DoNotSchedule</strong> (默认) 告诉调度器不要调度该 Pod，因此也可以叫作<strong>硬策略</strong>；</li>
<li><strong>ScheduleAnyway</strong> 告诉调度器根据每个 Node 的 skew 值打分排序后仍然调度，因此也可以叫作<strong>软策略</strong>。</li>
</ul>
</li>
</ul>
<h3 id="3-单个拓扑约束"><a href="#3-单个拓扑约束" class="headerlink" title="3.单个拓扑约束"></a>3.单个拓扑约束</h3><p>假设你拥有一个 4 节点集群，其中标记为 <code>foo:bar</code> 的 3 个 Pod 分别位于 node1、node2 和 node3 中：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325153609.png" alt="单个 TopologySpreadConstraint"></p>
<p>如果希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其约束：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">topologyKey:</span> <span class="string">zone</span></span><br><span class="line">    <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">    <span class="attr">labelSelector:</span></span><br><span class="line">      <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pause</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/pause:3.1</span></span><br></pre></td></tr></table></figure>

<p><code>topologyKey: zone</code> 意味着均匀分布将只应用于存在标签键值对为 <code>zone:&lt;any value&gt;</code> 的节点。 <code>whenUnsatisfiable: DoNotSchedule</code> 告诉调度器如果新的 Pod 不满足约束，则不可调度。如果调度器将新的 Pod 放入 “zoneA”，Pods 分布将变为 <code>[3, 1]</code>，因此实际的偏差为 <code>2(3 - 1)</code>，这违反了 <code>maxSkew: 1</code> 的约定。此示例中，新 Pod 只能放置在 “zoneB” 上：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325153809.png" alt="zoneB"></p>
<p>或者</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325153830.png" alt="zoneB"></p>
<p>你可以调整 Pod 约束以满足各种要求：</p>
<ul>
<li>将 <code>maxSkew</code> 更改为更大的值，比如 “2”，这样新的 Pod 也可以放在 “zoneA” 上。</li>
<li>将 <code>topologyKey</code> 更改为 “node”，以便将 Pod 均匀分布在节点上而不是区域中。 在上面的例子中，如果 <code>maxSkew</code> 保持为 “1”，那么传入的 Pod 只能放在 “node4” 上。</li>
<li>将 <code>whenUnsatisfiable: DoNotSchedule</code> 更改为 <code>whenUnsatisfiable: ScheduleAnyway</code>， 以确保新的 Pod 可以被调度。</li>
</ul>
<h3 id="4-多个拓扑约束"><a href="#4-多个拓扑约束" class="headerlink" title="4.多个拓扑约束"></a>4.多个拓扑约束</h3><p>上面是单个 Pod 拓扑分布约束的情况，下面的例子建立在前面例子的基础上来对多个 Pod 拓扑分布约束进行说明。假设你拥有一个 4 节点集群，其中 3 个标记为 <code>foo:bar</code> 的 Pod 分别位于 node1、node2 和 node3 上：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325154130.png" alt="多个 TopologySpreadConstraint"></p>
<p>我们可以使用 2 个 <code>TopologySpreadConstraint</code> 来控制 Pod 在区域和节点两个维度上的分布：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># two-constraints.yaml</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mypod</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">topologyKey:</span> <span class="string">zone</span></span><br><span class="line">    <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">    <span class="attr">labelSelector:</span></span><br><span class="line">      <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">topologyKey:</span> <span class="string">node</span></span><br><span class="line">    <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">    <span class="attr">labelSelector:</span></span><br><span class="line">      <span class="attr">matchLabels:</span></span><br><span class="line">        <span class="attr">foo:</span> <span class="string">bar</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">pause</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/pause:3.1</span></span><br></pre></td></tr></table></figure>

<p>在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在 “zoneB” 中；而在第二个约束中， 新的 Pod 只能放置在 “node4” 上，最后两个约束的结果加在一起，唯一可行的选择是放置 在 “node4” 上。</p>
<p>🍂 <strong>多个约束之间是可能存在冲突的</strong>，假设有一个跨越 2 个区域的 3 节点集群：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20210325154257.png" alt="冲突"></p>
<p>如果对集群应用 <code>two-constraints.yaml</code>，会发现 “mypod” 处于 <code>Pending</code> 状态，这是因为为了满足第一个约束，”mypod” 只能放在 “zoneB” 中，而第二个约束要求 “mypod” 只能放在 “node2” 上，Pod 调度无法满足这两种约束，所以就冲突了。</p>
<p>为了克服这种情况，你可以增加 <code>maxSkew</code> 或修改其中一个约束，让其使用 <code>whenUnsatisfiable: ScheduleAnyway</code>。</p>
<h3 id="5-与NodeSelector-NodeAffinity一起使用"><a href="#5-与NodeSelector-NodeAffinity一起使用" class="headerlink" title="5.与NodeSelector /NodeAffinity一起使用"></a>5.与NodeSelector /NodeAffinity一起使用</h3><p>仔细观察可能你会发现我们<strong>并没有类似于 <code>topologyValues</code> 的字段来限制 Pod 将被调度到哪些拓扑去</strong>，默认情况会搜索所有节点并按 <code>topologyKey</code> 对其进行分组。有时这可能不是理想的情况，比如假设有一个集群，其节点标记为 <code>env=prod</code>、<code>env=staging</code>和 <code>env=qa</code>，现在你想跨区域将 Pod 均匀地放置到 <code>qa</code> 环境中，是否可行?</p>
<p>答案是肯定的，我们可以结合 <code>NodeSelector</code> 或 <code>NodeAffinity</code> 一起使用，<code>PodTopologySpread</code> 会计算满足选择器的节点之间的传播约束。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220119154229.png" alt="高级用法-1"></p>
<p>如上图所示我们可以通过指定 <code>spec.affinity.nodeAffinity</code> 将<strong>搜索范围</strong>限制为 <code>qa</code> 环境，在该范围内 Pod 将被调度到一个满足 <code>topologySpreadConstraints</code> 的区域，这里就只能被调度到 <code>zone=zone2</code> 的节点上去了。</p>
<h3 id="6-集群默认约束"><a href="#6-集群默认约束" class="headerlink" title="6.集群默认约束"></a>6.集群默认约束</h3><p>除了为单个 Pod 设置拓扑分布约束，<strong>也可以为集群设置默认的拓扑分布约束</strong>，默认拓扑分布约束在且仅在以下条件满足 时才会应用到 Pod 上：</p>
<ul>
<li>Pod 没有在其 <code>.spec.topologySpreadConstraints</code> 设置任何约束；</li>
<li>Pod 隶属于某个服务、副本控制器、ReplicaSet 或 StatefulSet。</li>
</ul>
<p>你可以在 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/scheduling/config/#profiles">调度方案（Schedulingg Profile）</a>中将默认约束作为 <code>PodTopologySpread</code> 插件参数的一部分来进行设置。 约束的设置采用和前面 Pod 中的规范一致，只是 <code>labelSelector</code> 必须为空。配置的示例可能看起来像下面这个样子：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"></span><br><span class="line"><span class="attr">profiles:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">pluginConfig:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PodTopologySpread</span></span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">          <span class="attr">defaultConstraints:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">              <span class="attr">topologyKey:</span> <span class="string">topology.kubernetes.io/zone</span></span><br><span class="line">              <span class="attr">whenUnsatisfiable:</span> <span class="string">ScheduleAnyway</span></span><br><span class="line">          <span class="attr">defaultingType:</span> <span class="string">List</span></span><br></pre></td></tr></table></figure>



<p>🍂 不用 DaemonSet，如何使用 Deployment 是否实现同样的功能？</p>
<p>我们知道 DaemonSet 控制器的功能就是在每个节点上运行一个 Pod，如何要使用 Deployment 来实现，首先就要设置副本数量为节点数，比如我们这里加上 master 节点一共3个节点，则要设置3个副本，要在 master 节点上执行自然要添加容忍，那么要如何保证一个节点上只运行一个 Pod 呢？是不是前面的提到的 Pod 反亲和性就可以实现，<strong>以自己 Pod 的标签来进行过滤校验即可</strong>，新的 Pod 不能运行在一个已经具有该 Pod 的节点上，是不是就是一个节点只能运行一个？</p>
<ul>
<li>模拟的资源清单如下所示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 06-daemonset-deployment.yaml</span></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: mock-ds-demo</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: mock-ds-demo</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: mock-ds-demo</span><br><span class="line">    spec:</span><br><span class="line">      tolerations: <span class="comment">#加上容忍，这样才有机会运行在master节点上</span></span><br><span class="line">      - key: <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">        operator: <span class="string">&quot;Exists&quot;</span></span><br><span class="line">        effect: <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: ngpt</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:  <span class="comment"># pod反亲合性</span></span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:  <span class="comment"># 硬策略</span></span><br><span class="line">          - labelSelector:</span><br><span class="line">              matchExpressions:</span><br><span class="line">              - key: app   <span class="comment"># Pod的标签</span></span><br><span class="line">                operator: In</span><br><span class="line">                values: [<span class="string">&quot;mock-ds-demo&quot;</span>]</span><br><span class="line">            topologyKey: kubernetes.io/hostname  <span class="comment"># 以hostname为拓扑域，相当于每一个节点都是一个拓扑域</span></span><br></pre></td></tr></table></figure>



<ul>
<li>创建上面的资源清单验证：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 06-daemonset-deployment.yaml </span><br><span class="line">deployment.apps/mock-ds-demo created</span><br><span class="line">$ kubectl get po -owide</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE   IP             NODE      NOMINATED NODE   READINESS GATES</span><br><span class="line">mock-ds-demo-8694759c69-882fx   1/1     Running   0          18s   10.244.0.15    master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">mock-ds-demo-8694759c69-frfhx   1/1     Running   0          18s   10.244.1.108   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">mock-ds-demo-8694759c69-p5cjw   1/1     Running   0          18s   10.244.2.235   node2     &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>可以看到我们用 Deployment 部署的服务在每个节点上都运行了一个 Pod，实现的效果和 DaemonSet 是一致的。</p>
<p>🍂 同样的如果想在每个节点（或指定的一些节点）上运行2个（或多个）Pod 副本，如何实现？</p>
<p>DaemonSet 是在每个节点上运行1个 Pod 副本，<strong>显然我们去创建2个（或多个）DaemonSet 即可实现该目标</strong>，但是这不是一个好的接近方案，<strong>而 <code>PodAntiAffinity</code> 只能将一个 Pod 调度到某个拓扑域中去</strong>，所以都不能很好的来解决这个问题。</p>
<p>要实现这种更细粒度的控制，我们可以通过<strong>设置拓扑分布约束</strong>来进行调度，<strong>设置拓扑分布约束来将 Pod 分布到不同的拓扑域下</strong>，从而实现高可用性或节省成本，具体实现方式请看下文。</p>
<table><tr><td bgcolor=yellow>💘 实践：用拓扑分布约束来扩展daemonset功能(测试成功)-2022.5.17</td></tr></table>

<p>现在我们再去解决上节课留下的一个问题 - <strong>如果想在每个节点（或指定的一些节点）上运行2个（或多个）Pod 副本，如何实现？</strong></p>
<ul>
<li> 这里以我们的集群为例，加上 master 节点一共有3个节点，每个节点运行2个副本，总共就需要6个 Pod 副本，要在 master 节点上运行，则同样需要添加容忍，如果只想在一个节点上运行2个副本，则可以使用我们的拓扑分布约束来进行细粒度控制，对应的资源清单如下所示：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#01-daemonset2.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">topo-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">6</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">topo</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">topo</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">ngpt</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">topo</span></span><br></pre></td></tr></table></figure>

<p>这里我们重点需要关注的就是 <code>topologySpreadConstraints</code> 部分的配置，我们选择使用 <code>kubernetes.io/hostname</code> 为拓扑域，相当于就是3个节点都是独立的，<code>maxSkew: 1</code> 表示最大的分布不均匀度为1，所以只能出现的调度结果就是每个节点运行2个 Pod。</p>
<p>maxSkew=2这个相当于是软策略，不可取。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219104104873.png" alt="image-20220219104104873"></p>
<p>maxSkew=1这个满足需求。</p>
<p>maxSkew=1，这个是一轮轮往下调度的；(那么一般情况，maxSkew=1应该是比较常用的)</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220119162204.png" alt="解析"></p>
<ul>
<li>直接创建上面的资源即可验证：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f 01-daemonset2.yaml </span><br><span class="line">deployment.apps/topo-demo created</span><br><span class="line">$ kubectl get po -l app=topo -owide</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE   IP             NODE      NOMINATED NODE   READINESS GATES</span><br><span class="line">topo-demo-6bbf65d967-ctf66   1/1     Running   0          43s   10.244.1.110   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo-6bbf65d967-gkqpx   1/1     Running   0          43s   10.244.0.16    master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo-6bbf65d967-jsj4h   1/1     Running   0          43s   10.244.2.236   node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo-6bbf65d967-kc9x8   1/1     Running   0          43s   10.244.2.237   node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo-6bbf65d967-nr9b9   1/1     Running   0          43s   10.244.1.109   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo-6bbf65d967-wfzmf   1/1     Running   0          43s   10.244.0.17    master1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>可以看到符合我们的预期，每个节点上运行了2个 Pod 副本。</p>
<ul>
<li>如果是要求每个节点上运行3个 Pod 副本呢？大家也可以尝试去练习下。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#02-daemonset.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">topo-demo2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">9</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">topo2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">topo2</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span></span><br><span class="line">        <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">        <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">ngpt1</span></span><br><span class="line">      <span class="attr">topologySpreadConstraints:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">maxSkew:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">        <span class="attr">whenUnsatisfiable:</span> <span class="string">DoNotSchedule</span></span><br><span class="line">        <span class="attr">labelSelector:</span></span><br><span class="line">          <span class="attr">matchLabels:</span></span><br><span class="line">            <span class="attr">app:</span> <span class="string">topo2</span></span><br></pre></td></tr></table></figure>

<p>查看现象：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hg@LAPTOP-G8TUFE0T:/mnt/c/Users/hg/Desktop/yaml/2022.2.19-40.拓扑分布约束-实验代码$ kubectl apply -f 02-daemonset.yaml </span><br><span class="line">deployment.apps/topo-demo2 created</span><br><span class="line">hg@LAPTOP-G8TUFE0T:/mnt/c/Users/hg/Desktop/yaml/2022.2.19-40.拓扑分布约束-实验代码$ kubectl get po -l app=topo2 -owide</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE   IP             NODE      NOMINATED NODE   READINESS GATES</span><br><span class="line">topo-demo2-87d77dbbd-25wbw   1/1     Running   0          53s   10.244.2.240   node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-77gm8   1/1     Running   0          53s   10.244.1.113   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-97npc   1/1     Running   0          53s   10.244.0.20    master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-fspcq   1/1     Running   0          53s   10.244.1.111   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-k5llk   1/1     Running   0          53s   10.244.1.112   node1     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-pc24x   1/1     Running   0          53s   10.244.2.238   node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-rdmwn   1/1     Running   0          53s   10.244.2.239   node2     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-s4rjp   1/1     Running   0          53s   10.244.0.18    master1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">topo-demo2-87d77dbbd-xrwmk   1/1     Running   0          53s   10.244.0.19    master1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>实验结束。😘</p>
<h2 id="12、Descheduler"><a href="#12、Descheduler" class="headerlink" title="12、Descheduler"></a>12、Descheduler</h2><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219161107422.png" alt="image-20220219161107422"></p>
<h3 id="1、Descheduler"><a href="#1、Descheduler" class="headerlink" title="1、Descheduler"></a>1、Descheduler</h3><p>从 kube-scheduler 的角度来看，它是通过一系列算法计算出最佳节点运行 Pod，当出现新的 Pod 进行调度时，调度程序会根据其当时对 Kubernetes 集群的资源描述做出最佳调度决定，但是 <strong>Kubernetes 集群是非常动态的</strong>，由于整个集群范围内的变化，比如一个节点为了维护，我们先执行了驱逐操作，这个节点上的所有 Pod 会被驱逐到其他节点去，但是当我们维护完成后，之前的 Pod 并不会自动回到该节点上来，因为 <strong>Pod 一旦被绑定了节点是不会触发重新调度的</strong>，由于这些变化，<strong>Kubernetes 集群在一段时间内就可能会出现不均衡的状态，所以需要均衡器来重新平衡集群。</strong></p>
<p>当然我们可以去手动做一些集群的平衡，比如手动去删掉某些 Pod，触发重新调度就可以了，但是显然这是一个繁琐的过程，也不是解决问题的方式。<strong>为了解决实际运行中集群资源无法充分利用或浪费的问题</strong>，可以使用 <a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/descheduler">descheduler</a> 组件对集群的 Pod 进行调度优化，<code>descheduler</code> 可以根据一些规则和配置策略来帮助我们重新平衡集群状态，其核心原理是根据其策略配置找到可以被移除的 Pod 并驱逐它们，**<u>其本身并不会进行调度被驱逐的 Pod，而是依靠默认的调度器来实现</u>**，目前支持的策略有：</p>
<ul>
<li>RemoveDuplicates</li>
<li>LowNodeUtilization</li>
<li>RemovePodsViolatingInterPodAntiAffinity</li>
<li>RemovePodsViolatingNodeAffinity</li>
<li>RemovePodsViolatingNodeTaints</li>
<li>RemovePodsViolatingTopologySpreadConstraint</li>
<li>RemovePodsHavingTooManyRestarts</li>
<li>PodLifeTime</li>
</ul>
<p>这些策略都是可以启用或者禁用的，作为策略的一部分，也可以配置与策略相关的一些参数，<strong>默认情况下，所有策略都是启用的</strong>。另外，还有一些通用配置，如下：</p>
<ul>
<li><code>nodeSelector</code>：限制要处理的节点</li>
<li><code>evictLocalStoragePods</code>: 驱逐使用 LocalStorage 的 Pods</li>
<li><code>ignorePvcPods</code>: 是否忽略配置 PVC 的 Pods，默认是 False</li>
<li><code>maxNoOfPodsToEvictPerNode</code>：节点允许的最大驱逐 Pods 数</li>
</ul>
<p>我们可以通过如下所示的 <code>DeschedulerPolicy</code> 来配置：(<strong>这里是要用一个configmap来进行使用的。</strong>)</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">nodeSelector:</span> <span class="string">prod=dev</span></span><br><span class="line"><span class="attr">evictLocalStoragePods:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">maxNoOfPodsToEvictPerNode:</span> <span class="number">40</span></span><br><span class="line"><span class="attr">ignorePvcPods:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">strategies:</span>  <span class="comment"># 配置策略</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>🍂 kubernetes-sigs/descheduler官方网站</p>
<p><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/descheduler">https://github.com/kubernetes-sigs/descheduler</a></p>
<h3 id="2、安装"><a href="#2、安装" class="headerlink" title="2、安装"></a>2、安装</h3><table><tr><td bgcolor=yellow>💘 实践：descheduler安装(测试成功)-2022.5.17</td></tr></table>

<ul>
<li> <code>descheduler</code> 可以以 <code>Job</code>、<code>CronJob</code> 或者 <code>Deployment</code> 的形式运行在 k8s 集群内，同样我们可以使用 Helm Chart 来安装 <code>descheduler</code>：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/</span></span><br><span class="line"><span class="string">&quot;descheduler&quot;</span> has been added to your repositories</span><br><span class="line"></span><br><span class="line">[root@master1 ~]<span class="comment">#helm fetch descheduler/descheduler</span></span><br><span class="line">[root@master1 ~]<span class="comment">#tar xf descheduler-0.23.1.tgz </span></span><br></pre></td></tr></table></figure>

<p>通过 Helm Chart 我们可以配置 <code>descheduler</code> 以 <code>CronJob</code> 或者 <code>Deployment</code> 方式运行。:warning: 默认情况下 <code>descheduler</code> 会以一个 <code>critical pod</code> 运行，以避免被自己或者 kubelet 驱逐了，需要确保集群中有 <code>system-cluster-critical</code> 这个 Priorityclass：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#kubectl get priorityclass system-cluster-critical</span></span><br><span class="line">NAME                      VALUE        GLOBAL-DEFAULT   AGE</span><br><span class="line">system-cluster-critical   2000000000   <span class="literal">false</span>            111d</span><br></pre></td></tr></table></figure>

<p>:warning:  注意：descheduler和k8s版本问题</p>
<p><a target="_blank" rel="noopener" href="https://github.com/kubernetes-sigs/descheduler">https://github.com/kubernetes-sigs/descheduler</a></p>
<p>什么样的k8s集群版本就是用什么样的descheduler；</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219223059197.png" alt="image-20220219223059197"></p>
<ul>
<li>使用 Helm Chart 安装默认情况下会以 <code>CronJob</code> 的形式运行，执行周期为 <code>schedule: &quot;*/2 * * * *&quot;</code>，这样每隔两分钟会执行一次 <code>descheduler</code> 任务，默认的配置策略如下所示：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">descheduler</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">policy.yaml:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    apiVersion: &quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="string">    kind: &quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="string">    strategies:</span></span><br><span class="line"><span class="string">      LowNodeUtilization:</span></span><br><span class="line"><span class="string">        enabled: true</span></span><br><span class="line"><span class="string">        params:</span></span><br><span class="line"><span class="string">          nodeResourceUtilizationThresholds:</span></span><br><span class="line"><span class="string">            targetThresholds:</span></span><br><span class="line"><span class="string">              cpu: 50</span></span><br><span class="line"><span class="string">              memory: 50</span></span><br><span class="line"><span class="string">              pods: 50</span></span><br><span class="line"><span class="string">            thresholds:</span></span><br><span class="line"><span class="string">              cpu: 20</span></span><br><span class="line"><span class="string">              memory: 20</span></span><br><span class="line"><span class="string">              pods: 20</span></span><br><span class="line"><span class="string">      RemoveDuplicates:</span></span><br><span class="line"><span class="string">        enabled: true</span></span><br><span class="line"><span class="string">      RemovePodsViolatingInterPodAntiAffinity:</span></span><br><span class="line"><span class="string">        enabled: true</span></span><br><span class="line"><span class="string">      RemovePodsViolatingNodeAffinity:</span></span><br><span class="line"><span class="string">        enabled: true</span></span><br><span class="line"><span class="string">        params:</span></span><br><span class="line"><span class="string">          nodeAffinityType:</span></span><br><span class="line"><span class="string">          - requiredDuringSchedulingIgnoredDuringExecution</span></span><br><span class="line"><span class="string">      RemovePodsViolatingNodeTaints:</span></span><br><span class="line"><span class="string">        enabled: true</span></span><br></pre></td></tr></table></figure>



<ul>
<li>通过配置 <code>DeschedulerPolicy</code> 的 <code>strategies</code>，可以指定 <code>descheduler</code> 的执行策略，这些策略都是可以启用或禁用的。下面我们会详细介绍，这里我们使用默认策略即可，使用如下命令直接安装即可：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@master1 ~]<span class="comment">#helm upgrade --install descheduler descheduler/descheduler --set image.repository=cnych/descheduler,podSecurityPolicy.create=false -n kube-system</span></span><br><span class="line">Release <span class="string">&quot;descheduler&quot;</span> does not exist. Installing it now.</span><br><span class="line">NAME: descheduler</span><br><span class="line">LAST DEPLOYED: Sat Feb 19 17:05:31 2022</span><br><span class="line">NAMESPACE: kube-system</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">NOTES:</span><br><span class="line">Descheduler installed as a CronJob .   </span><br></pre></td></tr></table></figure>



<ul>
<li>部署完成后会创建一个 <code>CronJob</code> 资源对象来平衡集群状态：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl get cronjob -n kube-system</span><br><span class="line">NAME          SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">descheduler   */2 * * * *   False     1        27s             31s</span><br><span class="line">➜ kubectl get job -n kube-system</span><br><span class="line">NAME                   COMPLETIONS   DURATION   AGE</span><br><span class="line">descheduler-27378876   1/1           72s        79s</span><br><span class="line">➜ kubectl get pods -n kube-system -l job-name=descheduler-27378876</span><br><span class="line">NAME                            READY   STATUS      RESTARTS   AGE</span><br><span class="line">descheduler-27378876--1-btjmd   0/1     Completed   0          2m21s</span><br></pre></td></tr></table></figure>



<ul>
<li>正常情况下就会创建一个对应的 Job 来执行 <code>descheduler</code> 任务，我们可以通过查看日志可以了解做了哪些平衡操作：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">➜ kubectl logs -f descheduler-27378876--1-btjmd -n kube-system</span><br><span class="line">I0121 02:37:10.127266       1 named_certificates.go:53] <span class="string">&quot;Loaded SNI cert&quot;</span> index=0 certName=<span class="string">&quot;self-signed loopback&quot;</span> certDetail=<span class="string">&quot;\&quot;apiserver-loopback-client@1642732630\&quot; [serving] validServingFor=[apiserver-loopback-client] issuer=\&quot;apiserver-loopback-client-ca@1642732629\&quot; (2022-01-21 01:37:09 +0000 UTC to 2023-01-21 01:37:09 +0000 UTC (now=2022-01-21 02:37:10.127237 +0000 UTC))&quot;</span></span><br><span class="line">I0121 02:37:10.127324       1 secure_serving.go:195] Serving securely on [::]:10258</span><br><span class="line">I0121 02:37:10.127363       1 tlsconfig.go:240] <span class="string">&quot;Starting DynamicServingCertificateController&quot;</span></span><br><span class="line">I0121 02:37:10.138724       1 node.go:46] <span class="string">&quot;Node lister returned empty list, now fetch directly&quot;</span></span><br><span class="line">I0121 02:37:10.172264       1 nodeutilization.go:167] <span class="string">&quot;Node is overutilized&quot;</span> node=<span class="string">&quot;master1&quot;</span> usage=map[cpu:1225m memory:565Mi pods:16] usagePercentage=map[cpu:61.25 memory:15.391786081415567 pods:14.545454545454545]</span><br><span class="line">I0121 02:37:10.172313       1 nodeutilization.go:164] <span class="string">&quot;Node is underutilized&quot;</span> node=<span class="string">&quot;node1&quot;</span> usage=map[cpu:675m memory:735Mi pods:16] usagePercentage=map[cpu:16.875 memory:9.542007959787252 pods:14.545454545454545]</span><br><span class="line">I0121 02:37:10.172328       1 nodeutilization.go:170] <span class="string">&quot;Node is appropriately utilized&quot;</span> node=<span class="string">&quot;node2&quot;</span> usage=map[cpu:975m memory:1515Mi pods:15] usagePercentage=map[cpu:24.375 memory:19.66820054018583 pods:13.636363636363637]</span><br><span class="line">I0121 02:37:10.172340       1 lownodeutilization.go:100] <span class="string">&quot;Criteria for a node under utilization&quot;</span> CPU=20 Mem=20 Pods=20</span><br><span class="line">I0121 02:37:10.172346       1 lownodeutilization.go:101] <span class="string">&quot;Number of underutilized nodes&quot;</span> totalNumber=1</span><br><span class="line">I0121 02:37:10.172355       1 lownodeutilization.go:114] <span class="string">&quot;Criteria for a node above target utilization&quot;</span> CPU=50 Mem=50 Pods=50</span><br><span class="line">I0121 02:37:10.172360       1 lownodeutilization.go:115] <span class="string">&quot;Number of overutilized nodes&quot;</span> totalNumber=1</span><br><span class="line">I0121 02:37:10.172374       1 nodeutilization.go:223] <span class="string">&quot;Total capacity to be moved&quot;</span> CPU=1325 Mem=3267772416 Pods=39</span><br><span class="line">I0121 02:37:10.172399       1 nodeutilization.go:226] <span class="string">&quot;Evicting pods from node&quot;</span> node=<span class="string">&quot;master1&quot;</span> usage=map[cpu:1225m memory:565Mi pods:16]</span><br><span class="line">I0121 02:37:10.172485       1 nodeutilization.go:229] <span class="string">&quot;Pods on node&quot;</span> node=<span class="string">&quot;master1&quot;</span> allPods=16 nonRemovablePods=13 removablePods=3</span><br><span class="line">I0121 02:37:10.172495       1 nodeutilization.go:236] <span class="string">&quot;Evicting pods based on priority, if they have same priority, they&#x27;ll be evicted based on QoS tiers&quot;</span></span><br><span class="line">I0121 02:37:10.180353       1 evictions.go:130] <span class="string">&quot;Evicted pod&quot;</span> pod=<span class="string">&quot;default/topo-demo-6bbf65d967-lzlfh&quot;</span> reason=<span class="string">&quot;LowNodeUtilization&quot;</span></span><br><span class="line">I0121 02:37:10.181506       1 nodeutilization.go:269] <span class="string">&quot;Evicted pods&quot;</span> pod=<span class="string">&quot;default/topo-demo-6bbf65d967-lzlfh&quot;</span> err=&lt;nil&gt;</span><br><span class="line">I0121 02:37:10.181541       1 nodeutilization.go:294] <span class="string">&quot;Updated node usage&quot;</span> node=<span class="string">&quot;master1&quot;</span> CPU=1225 Mem=592445440 Pods=15</span><br><span class="line">I0121 02:37:10.182496       1 event.go:291] <span class="string">&quot;Event occurred&quot;</span> object=<span class="string">&quot;default/topo-demo-6bbf65d967-lzlfh&quot;</span> kind=<span class="string">&quot;Pod&quot;</span> apiVersion=<span class="string">&quot;v1&quot;</span> <span class="built_in">type</span>=<span class="string">&quot;Normal&quot;</span> reason=<span class="string">&quot;Descheduled&quot;</span> message=<span class="string">&quot;pod evicted by sigs.k8s.io/deschedulerLowNodeUtilization&quot;</span></span><br><span class="line">......</span><br></pre></td></tr></table></figure>

<p>从日志中我们就可以清晰的知道因为什么策略驱逐了哪些 Pods。</p>
<ul>
<li>descheduler安装出现的问题</li>
</ul>
<p>:warning: 要注意自己helm chart版本和老师转存的镜像是否一致，否则可能拉取镜像失败：</p>
<p>values.yaml内容：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219185204895.png" alt="image-20220219185204895"></p>
<p>Chart.yaml文件内容：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219185509176.png" alt="image-20220219185509176"></p>
<p>老师这里已经转存好的镜像如下：</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219185903544.png" alt="image-20220219185903544"></p>
<p>因此这里需要用如下命令来拉取：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">helm upgrade --install descheduler descheduler/descheduler --<span class="built_in">set</span> image.repository=cnych/descheduler,image.tag=v0.22.1,podSecurityPolicy.create=<span class="literal">false</span> -n kube-system</span><br><span class="line"></span><br><span class="line">[root@master1 ~]<span class="comment">#helm upgrade --install descheduler descheduler/descheduler --set image.repository=cnych/descheduler,image.tag=v0.22.1,podSecurityPolicy.create=false -n kube-system</span></span><br><span class="line">Release <span class="string">&quot;descheduler&quot;</span> does not exist. Installing it now.</span><br><span class="line">NAME: descheduler</span><br><span class="line">LAST DEPLOYED: Sat Feb 19 19:04:52 2022</span><br><span class="line">NAMESPACE: kube-system</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">NOTES:</span><br><span class="line">Descheduler installed as a CronJob .</span><br></pre></td></tr></table></figure>

<p>完美。</p>
<h3 id="3、PDB"><a href="#3、PDB" class="headerlink" title="3、PDB"></a>3、PDB</h3><p>由于使用 <code>descheduler</code> 会将 Pod 驱逐进行重调度，但是如果一个服务的所有副本都被驱逐的话，则可能导致该服务不可用。如果服务本身存在单点故障，驱逐的时候肯定就会造成服务不可用了，<strong>这种情况我们强烈建议使用反亲和性和多副本来避免单点故障</strong>。但是如果服务本身就被打散在多个节点上，这些 Pod 都被驱逐的话，这个时候也会造成服务不可用了，这种情况下我们可以通过配置 <code>PDB（PodDisruptionBudget）</code> <strong>对象来避免所有副本同时被删除，比如我们可以设置在驱逐的时候某应用最多只有一个副本不可用</strong>，则创建如下所示的资源清单即可：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pdb-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">maxUnavailable:</span> <span class="number">1</span>  <span class="comment"># 设置最多不可用的副本数量，或者使用 minAvailable，可以使用整数或百分比</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span>    <span class="comment"># 匹配Pod标签</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demo</span></span><br></pre></td></tr></table></figure>

<p>关于 PDB 的更多详细信息可以查看官方文档：<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/%E3%80%82">https://kubernetes.io/docs/tasks/run-application/configure-pdb/。</a></p>
<blockquote>
<p>:warning: 所以如果我们使用 <code>descheduler</code> 来重新平衡集群状态，那么我们强烈建议给应用创建一个对应的 <code>PodDisruptionBudget</code> 对象进行保护。</p>
</blockquote>
<h3 id="4、策略"><a href="#4、策略" class="headerlink" title="4、策略"></a>4、策略</h3><h4 id="1-PodLifeTime"><a href="#1-PodLifeTime" class="headerlink" title="1.PodLifeTime"></a>1.PodLifeTime</h4><p>该策略用于驱逐比 <code>maxPodLifeTimeSeconds</code> 更旧的 Pods；可以通过 <code>podStatusPhases</code> 来配置哪类状态的 Pods 会被驱逐。建议为每个应用程序创建一个 PDB，以确保应用程序的可用性。</p>
<p>比如我们可以配置如下所示的策略来驱逐运行超过7天的 Pod：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;PodLifeTime&quot;:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="attr">maxPodLifeTimeSeconds:</span> <span class="number">604800</span>  <span class="comment"># Pods 运行最多7天</span></span><br></pre></td></tr></table></figure>

<h4 id="2-RemoveDuplicates"><a href="#2-RemoveDuplicates" class="headerlink" title="2.RemoveDuplicates"></a>2.RemoveDuplicates</h4><p><strong>该策略确保只有一个和 Pod 关联的 RS、Deployment 或者 Job 资源对象运行在同一节点上。</strong>如果还有更多的 Pod 则将这些重复的 Pod 进行驱逐，以便更好地在集群中分散 Pod。如果某些节点由于某些原因崩溃了，这些节点上的 Pod 漂移到了其他节点，导致多个与 RS 关联的 Pod 在同一个节点上运行，就有可能发生这种情况，一旦出现故障的节点再次准备就绪，就可以启用该策略来驱逐这些重复的 Pod。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220124113540.png" alt="RemoveDuplicates"></p>
<p>配置策略的时候，可以指定参数 <code>excludeOwnerKinds</code> 用于排除类型，这些类型下的 Pod 不会被驱逐：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemoveDuplicates&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">       <span class="attr">removeDuplicates:</span></span><br><span class="line">         <span class="attr">excludeOwnerKinds:</span></span><br><span class="line">         <span class="bullet">-</span> <span class="string">&quot;ReplicaSet&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="3-LowNodeUtilization"><a href="#3-LowNodeUtilization" class="headerlink" title="3.LowNodeUtilization"></a>3.LowNodeUtilization</h4><blockquote>
<p>这个策略用的想对比较多一些。</p>
</blockquote>
<p>该策略主要用于查找未充分利用的节点，并从其他节点驱逐 Pod，以便 kube-scheudler 重新将它们调度到未充分利用的节点上。该策略的参数可以通过字段 <code>nodeResourceUtilizationThresholds</code> 进行配置。</p>
<p>节点的利用率不足可以通过配置 <code>thresholds</code> 阈值参数来确定，可以通过 CPU、内存和 Pods 数量的百分比进行配置。如果节点的使用率均低于所有阈值，则认为该节点未充分利用。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220124112813.png" alt="LowNodeUtilization"></p>
<ul>
<li>此外，还有一个可配置的阈值 <code>targetThresholds</code>，用于计算可能驱逐 Pods 的潜在节点，该参数也可以配置 CPU、内存以及 Pods 数量的百分比进行配置。<code>thresholds</code> 和 <code>targetThresholds</code> 可以根据你的集群需求进行动态调整，如下所示示例：</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;LowNodeUtilization&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">       <span class="attr">nodeResourceUtilizationThresholds:</span></span><br><span class="line">         <span class="attr">thresholds:</span> <span class="comment">#注意：这个是未充分利用节点</span></span><br><span class="line">           <span class="string">&quot;cpu&quot;</span> <span class="string">:</span> <span class="number">20</span></span><br><span class="line">           <span class="attr">&quot;memory&quot;:</span> <span class="number">20</span></span><br><span class="line">           <span class="attr">&quot;pods&quot;:</span> <span class="number">20</span></span><br><span class="line">         <span class="attr">targetThresholds:</span> <span class="comment">#注意：这个可能驱逐 Pods 的潜在节点</span></span><br><span class="line">           <span class="string">&quot;cpu&quot;</span> <span class="string">:</span> <span class="number">50</span></span><br><span class="line">           <span class="attr">&quot;memory&quot;:</span> <span class="number">50</span></span><br><span class="line">           <span class="attr">&quot;pods&quot;:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>:warning: 注意：这里可以考虑下用p8s来采集node的负载指标然后写在这里进行自定义；</p>
</blockquote>
<p>需要注意的是：</p>
<ul>
<li>仅支持以下三种资源类型：cpu、memory、pods</li>
<li><code>thresholds</code> 和 <code>targetThresholds</code> 必须配置相同的类型</li>
<li>参数值的访问是0-100（百分制）</li>
<li>相同的资源类型，<code>thresholds</code> 的配置不能高于 <code>targetThresholds</code> 的配置</li>
</ul>
<p>如果未指定任何资源类型，则默认是100%，以避免节点从未充分利用变为过度利用。</p>
<p>和 <code>LowNodeUtilization</code> 策略关联的另一个参数是 <code>numberOfNodes</code>，只有当未充分利用的节点数大于该配置值的时候，才可以配置该参数来激活该策略，**<u>该参数对于大型集群非常有用</u>**，其中有一些节点可能会频繁使用或短期使用不足，默认情况下，numberOfNodes 为0。</p>
<h4 id="4-RemovePodsViolatingInterPodAntiAffinity"><a href="#4-RemovePodsViolatingInterPodAntiAffinity" class="headerlink" title="4.RemovePodsViolatingInterPodAntiAffinity"></a>4.RemovePodsViolatingInterPodAntiAffinity</h4><p>该策略可以确保从节点中删除违反 Pod 反亲和性的 Pod，比如某个节点上有 podA 这个 Pod，并且 podB 和 podC（在同一个节点上运行）具有禁止它们在同一个节点上运行的反亲和性规则，则 podA 将被从该节点上驱逐，以便 podB 和 podC 运行正常运行。当 podB 和 podC 已经运行在节点上后，反亲和性规则被创建就会发送这样的问题。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220124113908.png" alt="RemovePodsViolatingInterPodAntiAffinity"></p>
<p>要禁用该策略，直接配置成 false 即可：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemovePodsViolatingInterPodAntiAffinity&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h4 id="5-RemovePodsViolatingNodeTaints"><a href="#5-RemovePodsViolatingNodeTaints" class="headerlink" title="5.RemovePodsViolatingNodeTaints"></a>5.RemovePodsViolatingNodeTaints</h4><p>该策略可以确保从节点中删除违反 <code>NoSchedule</code> 污点的 Pod，比如有一个名为 podA 的 Pod，通过配置容忍 <code>key=value:NoSchedule</code> 允许被调度到有该污点配置的节点上，如果节点的污点随后被更新或者删除了，则污点将不再被 Pods 的容忍满足，然后将被驱逐：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemovePodsViolatingNodeTaints&quot;:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h4 id="6-RemovePodsViolatingNodeAffinity"><a href="#6-RemovePodsViolatingNodeAffinity" class="headerlink" title="6.RemovePodsViolatingNodeAffinity"></a>6.RemovePodsViolatingNodeAffinity</h4><p>该策略确保从节点中删除违反节点亲和性的 Pod。比如名为 podA 的 Pod 被调度到了节点 nodeA，podA 在调度的时候满足了节点亲和性规则 <code>requiredDuringSchedulingIgnoredDuringExecution</code>，但是随着时间的推移，节点 nodeA 不再满足该规则了，那么如果另一个满足节点亲和性规则的节点 nodeB 可用，则 podA 将被从节点 nodeA 驱逐，如下所示的策略配置示例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemovePodsViolatingNodeAffinity&quot;:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">params:</span></span><br><span class="line">      <span class="attr">nodeAffinityType:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;requiredDuringSchedulingIgnoredDuringExecution&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="7-RemovePodsViolatingTopologySpreadConstraint"><a href="#7-RemovePodsViolatingTopologySpreadConstraint" class="headerlink" title="7.RemovePodsViolatingTopologySpreadConstraint"></a>7.RemovePodsViolatingTopologySpreadConstraint</h4><p>该策略确保从节点驱逐违反拓扑分布约束的 Pods，具体来说，它试图驱逐将拓扑域平衡到每个约束的 <code>maxSkew</code> 内所需的最小 Pod 数，不过该策略需要 k8s 版本高于1.18才能使用。</p>
<p>默认情况下，此策略仅处理硬约束，如果将参数 <code>includeSoftConstraints</code> 设置为 True，也将支持软约束。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemovePodsViolatingTopologySpreadConstraint&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">       <span class="attr">includeSoftConstraints:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<h4 id="8-RemovePodsHavingTooManyRestarts"><a href="#8-RemovePodsHavingTooManyRestarts" class="headerlink" title="8.RemovePodsHavingTooManyRestarts"></a>8.RemovePodsHavingTooManyRestarts</h4><p>该策略确保从节点中删除重启次数过多的 Pods，它的参数包括 <code>podRestartThreshold</code>（这是应将 Pod 逐出的重新启动次数），以及包括<code>InitContainers</code>，它确定在计算中是否应考虑初始化容器的重新启动，策略配置如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;RemovePodsHavingTooManyRestarts&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">       <span class="attr">podsHavingTooManyRestarts:</span></span><br><span class="line">         <span class="attr">podRestartThreshold:</span> <span class="number">100</span></span><br><span class="line">         <span class="attr">includingInitContainers:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<h3 id="5、Filter-Pods"><a href="#5、Filter-Pods" class="headerlink" title="5、Filter Pods"></a>5、Filter Pods</h3><p>在驱逐 Pods 的时候，有时并不需要所有 Pods 都被驱逐，<code>descheduler</code> 提供了两种主要的方式进行过滤：命名空间过滤和优先级过滤。</p>
<h4 id="1-命名空间过滤"><a href="#1-命名空间过滤" class="headerlink" title="1.命名空间过滤"></a>1.命名空间过滤</h4><p>该策略可以配置是包含还是排除某些名称空间。可以使用该策略的有：</p>
<ul>
<li>PodLifeTime</li>
<li>RemovePodsHavingTooManyRestarts</li>
<li>RemovePodsViolatingNodeTaints</li>
<li>RemovePodsViolatingNodeAffinity</li>
<li>RemovePodsViolatingInterPodAntiAffinity</li>
<li>RemoveDuplicates</li>
<li>RemovePodsViolatingTopologySpreadConstraint</li>
</ul>
<p>比如只驱逐某些命令空间下的 Pods，则可以使用 <code>include</code> 参数进行配置，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;PodLifeTime&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">        <span class="attr">podLifeTime:</span></span><br><span class="line">          <span class="attr">maxPodLifeTimeSeconds:</span> <span class="number">86400</span></span><br><span class="line">        <span class="attr">namespaces:</span></span><br><span class="line">          <span class="attr">include:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;namespace1&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;namespace2&quot;</span></span><br></pre></td></tr></table></figure>



<p>又或者要排除掉某些命令空间下的 Pods，则可以使用 <code>exclude</code> 参数配置，如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;PodLifeTime&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">        <span class="attr">podLifeTime:</span></span><br><span class="line">          <span class="attr">maxPodLifeTimeSeconds:</span> <span class="number">86400</span></span><br><span class="line">        <span class="attr">namespaces:</span></span><br><span class="line">          <span class="attr">exclude:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;namespace1&quot;</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">&quot;namespace2&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="2-优先级过滤"><a href="#2-优先级过滤" class="headerlink" title="2.优先级过滤"></a>2.优先级过滤</h4><p>所有策略都可以配置优先级阈值，只有在该阈值以下的 Pod 才会被驱逐。我们可以通过设置 <code>thresholdPriorityClassName</code>（将阈值设置为指定优先级类别的值）或 <code>thresholdPriority</code>（直接设置阈值）参数来指定该阈值。默认情况下，该阈值设置为 <code>system-cluster-critical</code> 这个 PriorityClass 类的值。</p>
<p>比如使用 <code>thresholdPriority</code>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;PodLifeTime&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">        <span class="attr">podLifeTime:</span></span><br><span class="line">          <span class="attr">maxPodLifeTimeSeconds:</span> <span class="number">86400</span></span><br><span class="line">        <span class="attr">thresholdPriority:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure>



<p>或者使用 <code>thresholdPriorityClassName</code> 进行过滤：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">&quot;descheduler/v1alpha1&quot;</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">&quot;DeschedulerPolicy&quot;</span></span><br><span class="line"><span class="attr">strategies:</span></span><br><span class="line">  <span class="attr">&quot;PodLifeTime&quot;:</span></span><br><span class="line">     <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">     <span class="attr">params:</span></span><br><span class="line">        <span class="attr">podLifeTime:</span></span><br><span class="line">          <span class="attr">maxPodLifeTimeSeconds:</span> <span class="number">86400</span></span><br><span class="line">        <span class="attr">thresholdPriorityClassName:</span> <span class="string">&quot;priorityclass1&quot;</span></span><br></pre></td></tr></table></figure>

<p>不过需要注意不能同时配置 <code>thresholdPriority</code> 和 <code>thresholdPriorityClassName</code>，如果指定的优先级类不存在，则 descheduler 不会创建它，并且会引发错误。</p>
<h3 id="6、注意事项"><a href="#6、注意事项" class="headerlink" title="6、注意事项"></a>6、注意事项</h3><p>当使用descheduler驱除Pods的时候，需要注意以下几点：</p>
<ul>
<li><p>关键性 Pod 不会被驱逐，比如 <code>priorityClassName</code> 设置为 <code>system-cluster-critical</code> 或 <code>system-node-critical</code> 的 Pod</p>
</li>
<li><p>不属于 RS、Deployment 或 Job 管理的 Pods 不会被驱逐</p>
</li>
<li><p>DaemonSet 创建的 Pods 不会被驱逐</p>
</li>
<li><p>使用 <code>LocalStorage</code> 的 Pod 不会被驱逐，除非设置 <code>evictLocalStoragePods: true</code></p>
</li>
<li><p>具有 PVC 的 Pods 不会被驱逐，除非设置 <code>ignorePvcPods: true</code></p>
</li>
<li><p>在 <code>LowNodeUtilization</code> 和 <code>RemovePodsViolatingInterPodAntiAffinity</code> 策略下，Pods 按优先级从低到高进行驱逐，如果优先级相同，<code>Besteffort</code> 类型的 Pod 要先于 <u><code>Burstable</code> 和 <code>Guaranteed</code> 类型</u>被驱逐</p>
</li>
<li><p><code>annotations</code> 中带有 <code>descheduler.alpha.kubernetes.io/evict</code> 字段的 Pod 都可以被驱逐，该注释用于覆盖阻止驱逐的检查，用户可以选择驱逐哪个Pods</p>
</li>
<li><p>如果 Pods 驱逐失败，可以设置 <code>--v=4</code> 从 <code>descheduler</code> 日志中查找原因，如果驱逐违反 PDB 约束，则不会驱逐这类 Pods</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220219222613564.png" alt="image-20220219222613564"></p>
</li>
</ul>
<h2 id="13、多调度器"><a href="#13、多调度器" class="headerlink" title="13、多调度器"></a>13、多调度器</h2><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514231504.png"></p>
<h2 id="14、动态调度器"><a href="#14、动态调度器" class="headerlink" title="14、动态调度器"></a>14、动态调度器</h2><p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514231822.png"><br>代码的实现而是很简单的。</p>
<p>🍂<br>docs.gorance.io</p>
<p>🍂<br><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220515101445.png"><br>p8s基本是k8s的标配！<br>这个调度插件只一个锦上添花的能力，而不会影响主调度能力的。</p>
<p>🍂<br><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220515101723.png"><br>webhook，节点超售–实际生产上有大规模使用的！  </p>
<p>🍂 动态调度成效</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20220514231958.png"></p>
<h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>我的博客主旨：</p>
<ul>
<li>排版美观，语言精炼；</li>
<li>文档即手册，步骤明细，拒绝埋坑，提供源码；</li>
<li>本人实战文档都是亲测成功的，各位小伙伴在实际操作过程中如有什么疑问，可随时联系本人帮您解决问题，让我们一起进步！</li>
</ul>
<ol>
<li><p>个人微信二维码：x2675263825 （舍得）， qq：2675263825。</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20211106144206.png" alt="image-20211002091450217"></p>
</li>
<li><p>个人微信公众号：《云原生架构师实战》</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20211106144224.png" alt="image-20211002141739664"></p>
</li>
<li><p>个人csdn</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_39246554?spm=1010.2135.3001.5421">https://blog.csdn.net/weixin_39246554?spm=1010.2135.3001.5421</a></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/20211106144230.png" alt="image-20211002092344616"></p>
</li>
<li><p>个人博客：(<a href="http://www.onlyyou520.com/">www.onlyyou520.com</a>)</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220513150311181.png" alt="image-20220513150311181"></p>
</li>
<li><p>开源干货😘</p>
<p>语雀：<a target="_blank" rel="noopener" href="https://www.yuque.com/go/doc/73723298#">https://www.yuque.com/go/doc/73723298?#</a></p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220513150633944.png" alt="image-20220513150633944"></p>
</li>
</ol>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>好了，关于本次就到这里了，感谢大家阅读，最后贴上我女神的photo，祝大家生活快乐，每天都过的有意义哦，我们下期见！</p>
<p><img src="https://bucket-hg.oss-cn-shanghai.aliyuncs.com/img/image-20220217191118541.png" alt="image-20220217191118541"></p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">一个人</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://www.onlyyou520.com/2022/05/17/157%20kubernetes%E8%B0%83%E5%BA%A6%E5%99%A8/">https://www.onlyyou520.com/2022/05/17/157%20kubernetes%E8%B0%83%E5%BA%A6%E5%99%A8/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">一个人</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E8%B0%83%E5%BA%A6%E5%99%A8/">
                                    <span class="chip bg-color">调度器</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2022/05/20/159%20%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%88%E5%8D%9A%E5%AE%A2%E5%88%86%E4%BA%AB%EF%BC%89-2022.5.20/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="golang-运算符">
                        
                        <span class="card-title">golang-运算符</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2022-05-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" class="post-category">
                                    编程语言
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/golang/">
                        <span class="chip bg-color">golang</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2022/05/17/158%20%E7%88%B1%E7%9A%84%E8%AA%93%E8%A8%80%E5%92%8C%E6%89%BF%E8%AF%BA/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/14.jpg" class="responsive-img" alt="k8s">
                        
                        <span class="card-title">k8s</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2022-05-17
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%99%88%E6%9E%9C%E7%9A%84%E5%B9%B8%E7%A6%8F%E5%93%B2%E5%AD%A6%E8%AF%BE/" class="post-category">
                                    陈果的幸福哲学课
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E9%99%88%E6%9E%9C%E7%9A%84%E5%B9%B8%E7%A6%8F%E5%93%B2%E5%AD%A6%E8%AF%BE/">
                        <span class="chip bg-color">陈果的幸福哲学课</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2022</span>
            
            <span id="year">2019</span>
            <a href="/about" target="_blank">一个人</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/OnlyOnexl" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:2675263825@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=2675263825" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 2675263825" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
